{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os, sys\n",
    "os.chdir(\"../scripts/diffuser_icons/\")\n",
    "from dataset import *\n",
    "from config import ConditionalTrainingConfig\n",
    "from collections import Counter, defaultdict, OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 soda\n",
      "30 ('âˆ¦', 'DejaVuSans')\n"
     ]
    }
   ],
   "source": [
    "max_num_objs = 30\n",
    "with open(\"../../data/nouns/all_nouns.txt\", \"r\") as f:\n",
    "    nouns = [x.strip() for x in f.readlines()][:max_num_objs]\n",
    "print(len(nouns), nouns[0])\n",
    "with open(\"../../data/matplotlib/unicode.jsonl\", \"r\", encoding=\"unicode-escape\") as f: \n",
    "    icons = [(json.loads(x)[0], json.loads(x)[2]) for x in f.readlines()][:max_num_objs]\n",
    "print(len(icons), icons[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3240 12780\n"
     ]
    }
   ],
   "source": [
    "canvas_size = (32, 32) #(128, 128)\n",
    "icon_size = 32 #128\n",
    "icon_fontsize = 28 #120\n",
    "split = \"split24\"\n",
    "#train_pairs, test_pairs = create_data_single_obj(nouns, icons, canvas_size=canvas_size, icon_size=icon_size, fontsize=fontsize)\n",
    "train_pairs, test_pairs = eval(f\"create_data_{split}\")(nouns, icons, canvas_size=canvas_size, icon_size=icon_size, fontsize=icon_fontsize)\n",
    "print(len(train_pairs), len(test_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a backpack is on top of a phone.\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAAgACADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD3+iivPZfiDrWr6ld23g3wudXtbOUwzX892sEJkHVUyPnx6j+RBoA7afVtPtdTtdNnvIY727DGCBm+eQKMsQPYVcrwbTNY8ba/8T9Q1638LWlzc6Pb/wBmtbHUFWOBySzEOfvN94HHTNe16PPqFzpNvNqtmlnfMuZrdJRIqHJ4DDrxigC467kZQSuQRkdq8g8EeMtJ8AeEbjwx4ikFhq+jeefKlBUXalmdXjPRt2cDvXsNcT8UPDF74r8LppenWlvJcT3MSSTy7Q0EO7LspPOeAMDqCaAOV+FPizwvovgwS6r4j02LVdRuZb68V51DB3bof+Agfma9XsL+01SxivbG4juLWUZjlibcrDOODWbF4N8MwwpEvh/SyqKFG6zjJwPU45rWtra3s7dLe1giggQYSOJAqqPYDgUAf//Z",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAIAAAD8GO2jAAACeUlEQVR4Ae1Vz4tpYRjmRlYozchIFmpQmsaPpYUsxEYUyVr+ALvZyMZ2Ss0fQCkLO0qSpWwUWdkgpdn6FWoM4T46t3NP53zn46rZ3W9xet/nfZ/n/X6935Gez2fJT45fPyl+0f5f4OoO//gWya5OQSxhu902Gg1EfT6fWq0WS5Pgmt43SqUSRJVK5W63oyjcv0WVSgUFAoGAQqEQnT4ClOKU0Pf3N+YOOtZBSUPozgL1eh3qcrl8uVzSC9x0yOv1ejAYQOt0OjG7kc/nYZhMpna7zSD4Pj09uVwu1v1j0OuPx+NoNIqZ8mkkPxKJCNVoK2i1WqFQaLVaQc1qtWq1WqlUCns2m2FBMDweD77ssNlsrP3XENZkkM/PT41Ggzyv1zscDrlp6XQa+OvrKxcUs0UPOZlMQsXtdu/3ex755eUFoUwmw8OJLrnA4XBgmrPb7fJok8kE6hi9Xo8XIrrkAjhbSDw8PAg5uVwOIYPBIAwREXInf319QYVpJRjcUa1W4eLwuSDNJpadz+fg4HZuNhtuAnCZ7HLxms0mF6fY5C0Cwel0Quj9/Z1LLhaLAFUqlfDkuWlcW7RAuVyGFh6yWq3GEtBKAOPxOItcNaTIAIc4EolEoVBAc0E3GAw+Pj7GYjH8BlKplN/vF1J0Op3dbufjlCkcj8e3tzdm0/k0kk98KmgrYERGoxHe5H6/j3dtsVjgwXh+fibpX3o+m83yQ5QVcEN4R/V6PcgfHx9c/Kotesg8ZqfTYaY2nU55IbpLbjT+MiUS5gfpcDiMRqMwSkH+rUA4HKZoEUO0/wFLwC/BYrGYzWZcUxa80bh+i24UEku7dYvE+Ffx38NSvDUaIpCKAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=32x32>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 84\n",
    "print(train_pairs[k][0])\n",
    "train_pairs[k][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('soap', 'cabinet')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nouns[23], nouns[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences = [x[0] for x in train_pairs]\n",
    "test_sentences = [x[0] for x in test_pairs]\n",
    "\"a cabinet is at the bottom of a soap.\" in train_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../diffuser_colored_sq/\")\n",
    "from model import T2IDiffusion\n",
    "from utils import *\n",
    "from torchsummary import summary\n",
    "from diffusers.utils.torch_utils import randn_tensor\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNet2DConditionalModel_with_posemb only_cross_attention=True\n",
      "Transformer2DModel_with_nonsquare_input only_cross_attention=True\n",
      "Transformer2DModel_with_nonsquare_input only_cross_attention=True\n",
      "Transformer2DModel_with_nonsquare_input only_cross_attention=True\n",
      "Transformer2DModel_with_nonsquare_input only_cross_attention=True\n",
      "Transformer2DModel_with_nonsquare_input only_cross_attention=True\n",
      "Transformer2DModel_with_nonsquare_input only_cross_attention=True\n",
      "Transformer2DModel_with_nonsquare_input only_cross_attention=True\n",
      "Transformer2DModel_with_nonsquare_input only_cross_attention=True\n",
      "Transformer2DModel_with_nonsquare_input only_cross_attention=True\n",
      "Transformer2DModel_with_nonsquare_input only_cross_attention=True\n",
      "Transformer2DModel_with_nonsquare_input only_cross_attention=True\n"
     ]
    }
   ],
   "source": [
    "config = ConditionalTrainingConfig()\n",
    "model = T2IDiffusion(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "image = randn_tensor((1, 3, 64, 32), device=device)\n",
    "encoder_hidden_states = randn_tensor((1, 8, 512))\n",
    "timesteps=torch.tensor([5])\n",
    "#summary(model.unet, [image, encoder_hidden_states, 0])\n",
    "# print(summary(\n",
    "#     model.unet,\n",
    "#     #input_data = [(2, 3, 64, 32), (1,), (2, 8, 512)], \n",
    "#     input_data = [image, timesteps, encoder_hidden_states],\n",
    "#     batch_dim = 0, \n",
    "#     dtypes=[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor], \n",
    "#     device=device,\n",
    "#     depth = 10, \n",
    "# ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------+----------+--------------------+\n",
      "|                                  Modules                                   | #Params  |    Param shape     |\n",
      "+----------------------------------------------------------------------------+----------+--------------------+\n",
      "|                            unet.conv_in.weight                             |   1728   |   [64, 3, 3, 3]    |\n",
      "|                             unet.conv_in.bias                              |    64    |        [64]        |\n",
      "|                    unet.time_embedding.linear_1.weight                     |  16384   |     [256, 64]      |\n",
      "|                     unet.time_embedding.linear_1.bias                      |   256    |       [256]        |\n",
      "|                    unet.time_embedding.linear_2.weight                     |  65536   |     [256, 256]     |\n",
      "|                     unet.time_embedding.linear_2.bias                      |   256    |       [256]        |\n",
      "|                        unet.encoder_hid_proj.weight                        |  65536   |     [128, 512]     |\n",
      "|                         unet.encoder_hid_proj.bias                         |   128    |       [128]        |\n",
      "|                 unet.down_blocks.0.resnets.0.norm1.weight                  |    64    |        [64]        |\n",
      "|                  unet.down_blocks.0.resnets.0.norm1.bias                   |    64    |        [64]        |\n",
      "|                 unet.down_blocks.0.resnets.0.conv1.weight                  |  36864   |   [64, 64, 3, 3]   |\n",
      "|                  unet.down_blocks.0.resnets.0.conv1.bias                   |    64    |        [64]        |\n",
      "|             unet.down_blocks.0.resnets.0.time_emb_proj.weight              |  16384   |     [64, 256]      |\n",
      "|              unet.down_blocks.0.resnets.0.time_emb_proj.bias               |    64    |        [64]        |\n",
      "|                 unet.down_blocks.0.resnets.0.norm2.weight                  |    64    |        [64]        |\n",
      "|                  unet.down_blocks.0.resnets.0.norm2.bias                   |    64    |        [64]        |\n",
      "|                 unet.down_blocks.0.resnets.0.conv2.weight                  |  36864   |   [64, 64, 3, 3]   |\n",
      "|                  unet.down_blocks.0.resnets.0.conv2.bias                   |    64    |        [64]        |\n",
      "|                 unet.down_blocks.0.resnets.1.norm1.weight                  |    64    |        [64]        |\n",
      "|                  unet.down_blocks.0.resnets.1.norm1.bias                   |    64    |        [64]        |\n",
      "|                 unet.down_blocks.0.resnets.1.conv1.weight                  |  36864   |   [64, 64, 3, 3]   |\n",
      "|                  unet.down_blocks.0.resnets.1.conv1.bias                   |    64    |        [64]        |\n",
      "|             unet.down_blocks.0.resnets.1.time_emb_proj.weight              |  16384   |     [64, 256]      |\n",
      "|              unet.down_blocks.0.resnets.1.time_emb_proj.bias               |    64    |        [64]        |\n",
      "|                 unet.down_blocks.0.resnets.1.norm2.weight                  |    64    |        [64]        |\n",
      "|                  unet.down_blocks.0.resnets.1.norm2.bias                   |    64    |        [64]        |\n",
      "|                 unet.down_blocks.0.resnets.1.conv2.weight                  |  36864   |   [64, 64, 3, 3]   |\n",
      "|                  unet.down_blocks.0.resnets.1.conv2.bias                   |    64    |        [64]        |\n",
      "|               unet.down_blocks.0.downsamplers.0.conv.weight                |  36864   |   [64, 64, 3, 3]   |\n",
      "|                unet.down_blocks.0.downsamplers.0.conv.bias                 |    64    |        [64]        |\n",
      "|     unet.down_blocks.1.attentions.0.transformer_blocks.0.norm1.weight      |   256    |       [256]        |\n",
      "|      unet.down_blocks.1.attentions.0.transformer_blocks.0.norm1.bias       |   256    |       [256]        |\n",
      "|   unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight   |  65536   |     [256, 256]     |\n",
      "|   unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight   |  32768   |     [256, 128]     |\n",
      "|   unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight   |  32768   |     [256, 128]     |\n",
      "| unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight |  65536   |     [256, 256]     |\n",
      "|  unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias  |   256    |       [256]        |\n",
      "|     unet.down_blocks.1.attentions.0.transformer_blocks.0.norm2.weight      |   256    |       [256]        |\n",
      "|      unet.down_blocks.1.attentions.0.transformer_blocks.0.norm2.bias       |   256    |       [256]        |\n",
      "|   unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight   |  65536   |     [256, 256]     |\n",
      "|   unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight   |  32768   |     [256, 128]     |\n",
      "|   unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight   |  32768   |     [256, 128]     |\n",
      "| unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight |  65536   |     [256, 256]     |\n",
      "|  unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias  |   256    |       [256]        |\n",
      "|     unet.down_blocks.1.attentions.0.transformer_blocks.0.norm3.weight      |   256    |       [256]        |\n",
      "|      unet.down_blocks.1.attentions.0.transformer_blocks.0.norm3.bias       |   256    |       [256]        |\n",
      "| unet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.weight  |  524288  |    [2048, 256]     |\n",
      "|  unet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias   |   2048   |       [2048]       |\n",
      "|    unet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.weight    |  262144  |    [256, 1024]     |\n",
      "|     unet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias     |   256    |       [256]        |\n",
      "|              unet.down_blocks.1.attentions.0.proj_out.weight               |  262144  |    [1024, 256]     |\n",
      "|               unet.down_blocks.1.attentions.0.proj_out.bias                |   1024   |       [1024]       |\n",
      "|           unet.down_blocks.1.attentions.0.pos_embed.proj.weight            |  262144  |  [256, 256, 2, 2]  |\n",
      "|            unet.down_blocks.1.attentions.0.pos_embed.proj.bias             |   256    |       [256]        |\n",
      "|              unet.down_blocks.1.attentions.0.norm_out.weight               |   256    |       [256]        |\n",
      "|               unet.down_blocks.1.attentions.0.norm_out.bias                |   256    |       [256]        |\n",
      "|     unet.down_blocks.1.attentions.1.transformer_blocks.0.norm1.weight      |   256    |       [256]        |\n",
      "|      unet.down_blocks.1.attentions.1.transformer_blocks.0.norm1.bias       |   256    |       [256]        |\n",
      "|   unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight   |  65536   |     [256, 256]     |\n",
      "|   unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight   |  32768   |     [256, 128]     |\n",
      "|   unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight   |  32768   |     [256, 128]     |\n",
      "| unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight |  65536   |     [256, 256]     |\n",
      "|  unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias  |   256    |       [256]        |\n",
      "|     unet.down_blocks.1.attentions.1.transformer_blocks.0.norm2.weight      |   256    |       [256]        |\n",
      "|      unet.down_blocks.1.attentions.1.transformer_blocks.0.norm2.bias       |   256    |       [256]        |\n",
      "|   unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight   |  65536   |     [256, 256]     |\n",
      "|   unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight   |  32768   |     [256, 128]     |\n",
      "|   unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight   |  32768   |     [256, 128]     |\n",
      "| unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight |  65536   |     [256, 256]     |\n",
      "|  unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias  |   256    |       [256]        |\n",
      "|     unet.down_blocks.1.attentions.1.transformer_blocks.0.norm3.weight      |   256    |       [256]        |\n",
      "|      unet.down_blocks.1.attentions.1.transformer_blocks.0.norm3.bias       |   256    |       [256]        |\n",
      "| unet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.weight  |  524288  |    [2048, 256]     |\n",
      "|  unet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias   |   2048   |       [2048]       |\n",
      "|    unet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.weight    |  262144  |    [256, 1024]     |\n",
      "|     unet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias     |   256    |       [256]        |\n",
      "|              unet.down_blocks.1.attentions.1.proj_out.weight               |  262144  |    [1024, 256]     |\n",
      "|               unet.down_blocks.1.attentions.1.proj_out.bias                |   1024   |       [1024]       |\n",
      "|           unet.down_blocks.1.attentions.1.pos_embed.proj.weight            |  262144  |  [256, 256, 2, 2]  |\n",
      "|            unet.down_blocks.1.attentions.1.pos_embed.proj.bias             |   256    |       [256]        |\n",
      "|              unet.down_blocks.1.attentions.1.norm_out.weight               |   256    |       [256]        |\n",
      "|               unet.down_blocks.1.attentions.1.norm_out.bias                |   256    |       [256]        |\n",
      "|                 unet.down_blocks.1.resnets.0.norm1.weight                  |    64    |        [64]        |\n",
      "|                  unet.down_blocks.1.resnets.0.norm1.bias                   |    64    |        [64]        |\n",
      "|                 unet.down_blocks.1.resnets.0.conv1.weight                  |  147456  |  [256, 64, 3, 3]   |\n",
      "|                  unet.down_blocks.1.resnets.0.conv1.bias                   |   256    |       [256]        |\n",
      "|             unet.down_blocks.1.resnets.0.time_emb_proj.weight              |  65536   |     [256, 256]     |\n",
      "|              unet.down_blocks.1.resnets.0.time_emb_proj.bias               |   256    |       [256]        |\n",
      "|                 unet.down_blocks.1.resnets.0.norm2.weight                  |   256    |       [256]        |\n",
      "|                  unet.down_blocks.1.resnets.0.norm2.bias                   |   256    |       [256]        |\n",
      "|                 unet.down_blocks.1.resnets.0.conv2.weight                  |  589824  |  [256, 256, 3, 3]  |\n",
      "|                  unet.down_blocks.1.resnets.0.conv2.bias                   |   256    |       [256]        |\n",
      "|             unet.down_blocks.1.resnets.0.conv_shortcut.weight              |  16384   |  [256, 64, 1, 1]   |\n",
      "|              unet.down_blocks.1.resnets.0.conv_shortcut.bias               |   256    |       [256]        |\n",
      "|                 unet.down_blocks.1.resnets.1.norm1.weight                  |   256    |       [256]        |\n",
      "|                  unet.down_blocks.1.resnets.1.norm1.bias                   |   256    |       [256]        |\n",
      "|                 unet.down_blocks.1.resnets.1.conv1.weight                  |  589824  |  [256, 256, 3, 3]  |\n",
      "|                  unet.down_blocks.1.resnets.1.conv1.bias                   |   256    |       [256]        |\n",
      "|             unet.down_blocks.1.resnets.1.time_emb_proj.weight              |  65536   |     [256, 256]     |\n",
      "|              unet.down_blocks.1.resnets.1.time_emb_proj.bias               |   256    |       [256]        |\n",
      "|                 unet.down_blocks.1.resnets.1.norm2.weight                  |   256    |       [256]        |\n",
      "|                  unet.down_blocks.1.resnets.1.norm2.bias                   |   256    |       [256]        |\n",
      "|                 unet.down_blocks.1.resnets.1.conv2.weight                  |  589824  |  [256, 256, 3, 3]  |\n",
      "|                  unet.down_blocks.1.resnets.1.conv2.bias                   |   256    |       [256]        |\n",
      "|               unet.down_blocks.1.downsamplers.0.conv.weight                |  589824  |  [256, 256, 3, 3]  |\n",
      "|                unet.down_blocks.1.downsamplers.0.conv.bias                 |   256    |       [256]        |\n",
      "|     unet.down_blocks.2.attentions.0.transformer_blocks.0.norm1.weight      |   1024   |       [1024]       |\n",
      "|      unet.down_blocks.2.attentions.0.transformer_blocks.0.norm1.bias       |   1024   |       [1024]       |\n",
      "|   unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.weight   | 1048576  |    [1024, 1024]    |\n",
      "|   unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.weight   |  131072  |    [1024, 128]     |\n",
      "|   unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.weight   |  131072  |    [1024, 128]     |\n",
      "| unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.weight | 1048576  |    [1024, 1024]    |\n",
      "|  unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias  |   1024   |       [1024]       |\n",
      "|     unet.down_blocks.2.attentions.0.transformer_blocks.0.norm2.weight      |   1024   |       [1024]       |\n",
      "|      unet.down_blocks.2.attentions.0.transformer_blocks.0.norm2.bias       |   1024   |       [1024]       |\n",
      "|   unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.weight   | 1048576  |    [1024, 1024]    |\n",
      "|   unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight   |  131072  |    [1024, 128]     |\n",
      "|   unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight   |  131072  |    [1024, 128]     |\n",
      "| unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.weight | 1048576  |    [1024, 1024]    |\n",
      "|  unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias  |   1024   |       [1024]       |\n",
      "|     unet.down_blocks.2.attentions.0.transformer_blocks.0.norm3.weight      |   1024   |       [1024]       |\n",
      "|      unet.down_blocks.2.attentions.0.transformer_blocks.0.norm3.bias       |   1024   |       [1024]       |\n",
      "| unet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.weight  | 8388608  |    [8192, 1024]    |\n",
      "|  unet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.bias   |   8192   |       [8192]       |\n",
      "|    unet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.weight    | 4194304  |    [1024, 4096]    |\n",
      "|     unet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.bias     |   1024   |       [1024]       |\n",
      "|              unet.down_blocks.2.attentions.0.proj_out.weight               | 4194304  |    [4096, 1024]    |\n",
      "|               unet.down_blocks.2.attentions.0.proj_out.bias                |   4096   |       [4096]       |\n",
      "|           unet.down_blocks.2.attentions.0.pos_embed.proj.weight            | 4194304  | [1024, 1024, 2, 2] |\n",
      "|            unet.down_blocks.2.attentions.0.pos_embed.proj.bias             |   1024   |       [1024]       |\n",
      "|              unet.down_blocks.2.attentions.0.norm_out.weight               |   1024   |       [1024]       |\n",
      "|               unet.down_blocks.2.attentions.0.norm_out.bias                |   1024   |       [1024]       |\n",
      "|     unet.down_blocks.2.attentions.1.transformer_blocks.0.norm1.weight      |   1024   |       [1024]       |\n",
      "|      unet.down_blocks.2.attentions.1.transformer_blocks.0.norm1.bias       |   1024   |       [1024]       |\n",
      "|   unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.weight   | 1048576  |    [1024, 1024]    |\n",
      "|   unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.weight   |  131072  |    [1024, 128]     |\n",
      "|   unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.weight   |  131072  |    [1024, 128]     |\n",
      "| unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.weight | 1048576  |    [1024, 1024]    |\n",
      "|  unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias  |   1024   |       [1024]       |\n",
      "|     unet.down_blocks.2.attentions.1.transformer_blocks.0.norm2.weight      |   1024   |       [1024]       |\n",
      "|      unet.down_blocks.2.attentions.1.transformer_blocks.0.norm2.bias       |   1024   |       [1024]       |\n",
      "|   unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.weight   | 1048576  |    [1024, 1024]    |\n",
      "|   unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight   |  131072  |    [1024, 128]     |\n",
      "|   unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight   |  131072  |    [1024, 128]     |\n",
      "| unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.weight | 1048576  |    [1024, 1024]    |\n",
      "|  unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias  |   1024   |       [1024]       |\n",
      "|     unet.down_blocks.2.attentions.1.transformer_blocks.0.norm3.weight      |   1024   |       [1024]       |\n",
      "|      unet.down_blocks.2.attentions.1.transformer_blocks.0.norm3.bias       |   1024   |       [1024]       |\n",
      "| unet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.weight  | 8388608  |    [8192, 1024]    |\n",
      "|  unet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.bias   |   8192   |       [8192]       |\n",
      "|    unet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.weight    | 4194304  |    [1024, 4096]    |\n",
      "|     unet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.bias     |   1024   |       [1024]       |\n",
      "|              unet.down_blocks.2.attentions.1.proj_out.weight               | 4194304  |    [4096, 1024]    |\n",
      "|               unet.down_blocks.2.attentions.1.proj_out.bias                |   4096   |       [4096]       |\n",
      "|           unet.down_blocks.2.attentions.1.pos_embed.proj.weight            | 4194304  | [1024, 1024, 2, 2] |\n",
      "|            unet.down_blocks.2.attentions.1.pos_embed.proj.bias             |   1024   |       [1024]       |\n",
      "|              unet.down_blocks.2.attentions.1.norm_out.weight               |   1024   |       [1024]       |\n",
      "|               unet.down_blocks.2.attentions.1.norm_out.bias                |   1024   |       [1024]       |\n",
      "|                 unet.down_blocks.2.resnets.0.norm1.weight                  |   256    |       [256]        |\n",
      "|                  unet.down_blocks.2.resnets.0.norm1.bias                   |   256    |       [256]        |\n",
      "|                 unet.down_blocks.2.resnets.0.conv1.weight                  | 2359296  | [1024, 256, 3, 3]  |\n",
      "|                  unet.down_blocks.2.resnets.0.conv1.bias                   |   1024   |       [1024]       |\n",
      "|             unet.down_blocks.2.resnets.0.time_emb_proj.weight              |  262144  |    [1024, 256]     |\n",
      "|              unet.down_blocks.2.resnets.0.time_emb_proj.bias               |   1024   |       [1024]       |\n",
      "|                 unet.down_blocks.2.resnets.0.norm2.weight                  |   1024   |       [1024]       |\n",
      "|                  unet.down_blocks.2.resnets.0.norm2.bias                   |   1024   |       [1024]       |\n",
      "|                 unet.down_blocks.2.resnets.0.conv2.weight                  | 9437184  | [1024, 1024, 3, 3] |\n",
      "|                  unet.down_blocks.2.resnets.0.conv2.bias                   |   1024   |       [1024]       |\n",
      "|             unet.down_blocks.2.resnets.0.conv_shortcut.weight              |  262144  | [1024, 256, 1, 1]  |\n",
      "|              unet.down_blocks.2.resnets.0.conv_shortcut.bias               |   1024   |       [1024]       |\n",
      "|                 unet.down_blocks.2.resnets.1.norm1.weight                  |   1024   |       [1024]       |\n",
      "|                  unet.down_blocks.2.resnets.1.norm1.bias                   |   1024   |       [1024]       |\n",
      "|                 unet.down_blocks.2.resnets.1.conv1.weight                  | 9437184  | [1024, 1024, 3, 3] |\n",
      "|                  unet.down_blocks.2.resnets.1.conv1.bias                   |   1024   |       [1024]       |\n",
      "|             unet.down_blocks.2.resnets.1.time_emb_proj.weight              |  262144  |    [1024, 256]     |\n",
      "|              unet.down_blocks.2.resnets.1.time_emb_proj.bias               |   1024   |       [1024]       |\n",
      "|                 unet.down_blocks.2.resnets.1.norm2.weight                  |   1024   |       [1024]       |\n",
      "|                  unet.down_blocks.2.resnets.1.norm2.bias                   |   1024   |       [1024]       |\n",
      "|                 unet.down_blocks.2.resnets.1.conv2.weight                  | 9437184  | [1024, 1024, 3, 3] |\n",
      "|                  unet.down_blocks.2.resnets.1.conv2.bias                   |   1024   |       [1024]       |\n",
      "|      unet.up_blocks.0.attentions.0.transformer_blocks.0.norm1.weight       |   1024   |       [1024]       |\n",
      "|       unet.up_blocks.0.attentions.0.transformer_blocks.0.norm1.bias        |   1024   |       [1024]       |\n",
      "|    unet.up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q.weight    | 1048576  |    [1024, 1024]    |\n",
      "|    unet.up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_k.weight    |  131072  |    [1024, 128]     |\n",
      "|    unet.up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_v.weight    |  131072  |    [1024, 128]     |\n",
      "|  unet.up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.weight  | 1048576  |    [1024, 1024]    |\n",
      "|   unet.up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.bias   |   1024   |       [1024]       |\n",
      "|      unet.up_blocks.0.attentions.0.transformer_blocks.0.norm2.weight       |   1024   |       [1024]       |\n",
      "|       unet.up_blocks.0.attentions.0.transformer_blocks.0.norm2.bias        |   1024   |       [1024]       |\n",
      "|    unet.up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_q.weight    | 1048576  |    [1024, 1024]    |\n",
      "|    unet.up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k.weight    |  131072  |    [1024, 128]     |\n",
      "|    unet.up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_v.weight    |  131072  |    [1024, 128]     |\n",
      "|  unet.up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.weight  | 1048576  |    [1024, 1024]    |\n",
      "|   unet.up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.bias   |   1024   |       [1024]       |\n",
      "|      unet.up_blocks.0.attentions.0.transformer_blocks.0.norm3.weight       |   1024   |       [1024]       |\n",
      "|       unet.up_blocks.0.attentions.0.transformer_blocks.0.norm3.bias        |   1024   |       [1024]       |\n",
      "|  unet.up_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.weight   | 8388608  |    [8192, 1024]    |\n",
      "|   unet.up_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.bias    |   8192   |       [8192]       |\n",
      "|     unet.up_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.weight     | 4194304  |    [1024, 4096]    |\n",
      "|      unet.up_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.bias      |   1024   |       [1024]       |\n",
      "|               unet.up_blocks.0.attentions.0.proj_out.weight                | 4194304  |    [4096, 1024]    |\n",
      "|                unet.up_blocks.0.attentions.0.proj_out.bias                 |   4096   |       [4096]       |\n",
      "|            unet.up_blocks.0.attentions.0.pos_embed.proj.weight             | 4194304  | [1024, 1024, 2, 2] |\n",
      "|             unet.up_blocks.0.attentions.0.pos_embed.proj.bias              |   1024   |       [1024]       |\n",
      "|               unet.up_blocks.0.attentions.0.norm_out.weight                |   1024   |       [1024]       |\n",
      "|                unet.up_blocks.0.attentions.0.norm_out.bias                 |   1024   |       [1024]       |\n",
      "|      unet.up_blocks.0.attentions.1.transformer_blocks.0.norm1.weight       |   1024   |       [1024]       |\n",
      "|       unet.up_blocks.0.attentions.1.transformer_blocks.0.norm1.bias        |   1024   |       [1024]       |\n",
      "|    unet.up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_q.weight    | 1048576  |    [1024, 1024]    |\n",
      "|    unet.up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_k.weight    |  131072  |    [1024, 128]     |\n",
      "|    unet.up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_v.weight    |  131072  |    [1024, 128]     |\n",
      "|  unet.up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.weight  | 1048576  |    [1024, 1024]    |\n",
      "|   unet.up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.bias   |   1024   |       [1024]       |\n",
      "|      unet.up_blocks.0.attentions.1.transformer_blocks.0.norm2.weight       |   1024   |       [1024]       |\n",
      "|       unet.up_blocks.0.attentions.1.transformer_blocks.0.norm2.bias        |   1024   |       [1024]       |\n",
      "|    unet.up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_q.weight    | 1048576  |    [1024, 1024]    |\n",
      "|    unet.up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_k.weight    |  131072  |    [1024, 128]     |\n",
      "|    unet.up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_v.weight    |  131072  |    [1024, 128]     |\n",
      "|  unet.up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.weight  | 1048576  |    [1024, 1024]    |\n",
      "|   unet.up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.bias   |   1024   |       [1024]       |\n",
      "|      unet.up_blocks.0.attentions.1.transformer_blocks.0.norm3.weight       |   1024   |       [1024]       |\n",
      "|       unet.up_blocks.0.attentions.1.transformer_blocks.0.norm3.bias        |   1024   |       [1024]       |\n",
      "|  unet.up_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.weight   | 8388608  |    [8192, 1024]    |\n",
      "|   unet.up_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.bias    |   8192   |       [8192]       |\n",
      "|     unet.up_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.weight     | 4194304  |    [1024, 4096]    |\n",
      "|      unet.up_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.bias      |   1024   |       [1024]       |\n",
      "|               unet.up_blocks.0.attentions.1.proj_out.weight                | 4194304  |    [4096, 1024]    |\n",
      "|                unet.up_blocks.0.attentions.1.proj_out.bias                 |   4096   |       [4096]       |\n",
      "|            unet.up_blocks.0.attentions.1.pos_embed.proj.weight             | 4194304  | [1024, 1024, 2, 2] |\n",
      "|             unet.up_blocks.0.attentions.1.pos_embed.proj.bias              |   1024   |       [1024]       |\n",
      "|               unet.up_blocks.0.attentions.1.norm_out.weight                |   1024   |       [1024]       |\n",
      "|                unet.up_blocks.0.attentions.1.norm_out.bias                 |   1024   |       [1024]       |\n",
      "|      unet.up_blocks.0.attentions.2.transformer_blocks.0.norm1.weight       |   1024   |       [1024]       |\n",
      "|       unet.up_blocks.0.attentions.2.transformer_blocks.0.norm1.bias        |   1024   |       [1024]       |\n",
      "|    unet.up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_q.weight    | 1048576  |    [1024, 1024]    |\n",
      "|    unet.up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_k.weight    |  131072  |    [1024, 128]     |\n",
      "|    unet.up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_v.weight    |  131072  |    [1024, 128]     |\n",
      "|  unet.up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_out.0.weight  | 1048576  |    [1024, 1024]    |\n",
      "|   unet.up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_out.0.bias   |   1024   |       [1024]       |\n",
      "|      unet.up_blocks.0.attentions.2.transformer_blocks.0.norm2.weight       |   1024   |       [1024]       |\n",
      "|       unet.up_blocks.0.attentions.2.transformer_blocks.0.norm2.bias        |   1024   |       [1024]       |\n",
      "|    unet.up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_q.weight    | 1048576  |    [1024, 1024]    |\n",
      "|    unet.up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_k.weight    |  131072  |    [1024, 128]     |\n",
      "|    unet.up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_v.weight    |  131072  |    [1024, 128]     |\n",
      "|  unet.up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_out.0.weight  | 1048576  |    [1024, 1024]    |\n",
      "|   unet.up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_out.0.bias   |   1024   |       [1024]       |\n",
      "|      unet.up_blocks.0.attentions.2.transformer_blocks.0.norm3.weight       |   1024   |       [1024]       |\n",
      "|       unet.up_blocks.0.attentions.2.transformer_blocks.0.norm3.bias        |   1024   |       [1024]       |\n",
      "|  unet.up_blocks.0.attentions.2.transformer_blocks.0.ff.net.0.proj.weight   | 8388608  |    [8192, 1024]    |\n",
      "|   unet.up_blocks.0.attentions.2.transformer_blocks.0.ff.net.0.proj.bias    |   8192   |       [8192]       |\n",
      "|     unet.up_blocks.0.attentions.2.transformer_blocks.0.ff.net.2.weight     | 4194304  |    [1024, 4096]    |\n",
      "|      unet.up_blocks.0.attentions.2.transformer_blocks.0.ff.net.2.bias      |   1024   |       [1024]       |\n",
      "|               unet.up_blocks.0.attentions.2.proj_out.weight                | 4194304  |    [4096, 1024]    |\n",
      "|                unet.up_blocks.0.attentions.2.proj_out.bias                 |   4096   |       [4096]       |\n",
      "|            unet.up_blocks.0.attentions.2.pos_embed.proj.weight             | 4194304  | [1024, 1024, 2, 2] |\n",
      "|             unet.up_blocks.0.attentions.2.pos_embed.proj.bias              |   1024   |       [1024]       |\n",
      "|               unet.up_blocks.0.attentions.2.norm_out.weight                |   1024   |       [1024]       |\n",
      "|                unet.up_blocks.0.attentions.2.norm_out.bias                 |   1024   |       [1024]       |\n",
      "|                  unet.up_blocks.0.resnets.0.norm1.weight                   |   2048   |       [2048]       |\n",
      "|                   unet.up_blocks.0.resnets.0.norm1.bias                    |   2048   |       [2048]       |\n",
      "|                  unet.up_blocks.0.resnets.0.conv1.weight                   | 18874368 | [1024, 2048, 3, 3] |\n",
      "|                   unet.up_blocks.0.resnets.0.conv1.bias                    |   1024   |       [1024]       |\n",
      "|              unet.up_blocks.0.resnets.0.time_emb_proj.weight               |  262144  |    [1024, 256]     |\n",
      "|               unet.up_blocks.0.resnets.0.time_emb_proj.bias                |   1024   |       [1024]       |\n",
      "|                  unet.up_blocks.0.resnets.0.norm2.weight                   |   1024   |       [1024]       |\n",
      "|                   unet.up_blocks.0.resnets.0.norm2.bias                    |   1024   |       [1024]       |\n",
      "|                  unet.up_blocks.0.resnets.0.conv2.weight                   | 9437184  | [1024, 1024, 3, 3] |\n",
      "|                   unet.up_blocks.0.resnets.0.conv2.bias                    |   1024   |       [1024]       |\n",
      "|              unet.up_blocks.0.resnets.0.conv_shortcut.weight               | 2097152  | [1024, 2048, 1, 1] |\n",
      "|               unet.up_blocks.0.resnets.0.conv_shortcut.bias                |   1024   |       [1024]       |\n",
      "|                  unet.up_blocks.0.resnets.1.norm1.weight                   |   2048   |       [2048]       |\n",
      "|                   unet.up_blocks.0.resnets.1.norm1.bias                    |   2048   |       [2048]       |\n",
      "|                  unet.up_blocks.0.resnets.1.conv1.weight                   | 18874368 | [1024, 2048, 3, 3] |\n",
      "|                   unet.up_blocks.0.resnets.1.conv1.bias                    |   1024   |       [1024]       |\n",
      "|              unet.up_blocks.0.resnets.1.time_emb_proj.weight               |  262144  |    [1024, 256]     |\n",
      "|               unet.up_blocks.0.resnets.1.time_emb_proj.bias                |   1024   |       [1024]       |\n",
      "|                  unet.up_blocks.0.resnets.1.norm2.weight                   |   1024   |       [1024]       |\n",
      "|                   unet.up_blocks.0.resnets.1.norm2.bias                    |   1024   |       [1024]       |\n",
      "|                  unet.up_blocks.0.resnets.1.conv2.weight                   | 9437184  | [1024, 1024, 3, 3] |\n",
      "|                   unet.up_blocks.0.resnets.1.conv2.bias                    |   1024   |       [1024]       |\n",
      "|              unet.up_blocks.0.resnets.1.conv_shortcut.weight               | 2097152  | [1024, 2048, 1, 1] |\n",
      "|               unet.up_blocks.0.resnets.1.conv_shortcut.bias                |   1024   |       [1024]       |\n",
      "|                  unet.up_blocks.0.resnets.2.norm1.weight                   |   1280   |       [1280]       |\n",
      "|                   unet.up_blocks.0.resnets.2.norm1.bias                    |   1280   |       [1280]       |\n",
      "|                  unet.up_blocks.0.resnets.2.conv1.weight                   | 11796480 | [1024, 1280, 3, 3] |\n",
      "|                   unet.up_blocks.0.resnets.2.conv1.bias                    |   1024   |       [1024]       |\n",
      "|              unet.up_blocks.0.resnets.2.time_emb_proj.weight               |  262144  |    [1024, 256]     |\n",
      "|               unet.up_blocks.0.resnets.2.time_emb_proj.bias                |   1024   |       [1024]       |\n",
      "|                  unet.up_blocks.0.resnets.2.norm2.weight                   |   1024   |       [1024]       |\n",
      "|                   unet.up_blocks.0.resnets.2.norm2.bias                    |   1024   |       [1024]       |\n",
      "|                  unet.up_blocks.0.resnets.2.conv2.weight                   | 9437184  | [1024, 1024, 3, 3] |\n",
      "|                   unet.up_blocks.0.resnets.2.conv2.bias                    |   1024   |       [1024]       |\n",
      "|              unet.up_blocks.0.resnets.2.conv_shortcut.weight               | 1310720  | [1024, 1280, 1, 1] |\n",
      "|               unet.up_blocks.0.resnets.2.conv_shortcut.bias                |   1024   |       [1024]       |\n",
      "|                 unet.up_blocks.0.upsamplers.0.conv.weight                  | 9437184  | [1024, 1024, 3, 3] |\n",
      "|                  unet.up_blocks.0.upsamplers.0.conv.bias                   |   1024   |       [1024]       |\n",
      "|      unet.up_blocks.1.attentions.0.transformer_blocks.0.norm1.weight       |   256    |       [256]        |\n",
      "|       unet.up_blocks.1.attentions.0.transformer_blocks.0.norm1.bias        |   256    |       [256]        |\n",
      "|    unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight    |  65536   |     [256, 256]     |\n",
      "|    unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight    |  32768   |     [256, 128]     |\n",
      "|    unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight    |  32768   |     [256, 128]     |\n",
      "|  unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight  |  65536   |     [256, 256]     |\n",
      "|   unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias   |   256    |       [256]        |\n",
      "|      unet.up_blocks.1.attentions.0.transformer_blocks.0.norm2.weight       |   256    |       [256]        |\n",
      "|       unet.up_blocks.1.attentions.0.transformer_blocks.0.norm2.bias        |   256    |       [256]        |\n",
      "|    unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight    |  65536   |     [256, 256]     |\n",
      "|    unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight    |  32768   |     [256, 128]     |\n",
      "|    unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight    |  32768   |     [256, 128]     |\n",
      "|  unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight  |  65536   |     [256, 256]     |\n",
      "|   unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias   |   256    |       [256]        |\n",
      "|      unet.up_blocks.1.attentions.0.transformer_blocks.0.norm3.weight       |   256    |       [256]        |\n",
      "|       unet.up_blocks.1.attentions.0.transformer_blocks.0.norm3.bias        |   256    |       [256]        |\n",
      "|  unet.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.weight   |  524288  |    [2048, 256]     |\n",
      "|   unet.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias    |   2048   |       [2048]       |\n",
      "|     unet.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.weight     |  262144  |    [256, 1024]     |\n",
      "|      unet.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias      |   256    |       [256]        |\n",
      "|               unet.up_blocks.1.attentions.0.proj_out.weight                |  262144  |    [1024, 256]     |\n",
      "|                unet.up_blocks.1.attentions.0.proj_out.bias                 |   1024   |       [1024]       |\n",
      "|            unet.up_blocks.1.attentions.0.pos_embed.proj.weight             |  262144  |  [256, 256, 2, 2]  |\n",
      "|             unet.up_blocks.1.attentions.0.pos_embed.proj.bias              |   256    |       [256]        |\n",
      "|               unet.up_blocks.1.attentions.0.norm_out.weight                |   256    |       [256]        |\n",
      "|                unet.up_blocks.1.attentions.0.norm_out.bias                 |   256    |       [256]        |\n",
      "|      unet.up_blocks.1.attentions.1.transformer_blocks.0.norm1.weight       |   256    |       [256]        |\n",
      "|       unet.up_blocks.1.attentions.1.transformer_blocks.0.norm1.bias        |   256    |       [256]        |\n",
      "|    unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight    |  65536   |     [256, 256]     |\n",
      "|    unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight    |  32768   |     [256, 128]     |\n",
      "|    unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight    |  32768   |     [256, 128]     |\n",
      "|  unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight  |  65536   |     [256, 256]     |\n",
      "|   unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias   |   256    |       [256]        |\n",
      "|      unet.up_blocks.1.attentions.1.transformer_blocks.0.norm2.weight       |   256    |       [256]        |\n",
      "|       unet.up_blocks.1.attentions.1.transformer_blocks.0.norm2.bias        |   256    |       [256]        |\n",
      "|    unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight    |  65536   |     [256, 256]     |\n",
      "|    unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight    |  32768   |     [256, 128]     |\n",
      "|    unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight    |  32768   |     [256, 128]     |\n",
      "|  unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight  |  65536   |     [256, 256]     |\n",
      "|   unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias   |   256    |       [256]        |\n",
      "|      unet.up_blocks.1.attentions.1.transformer_blocks.0.norm3.weight       |   256    |       [256]        |\n",
      "|       unet.up_blocks.1.attentions.1.transformer_blocks.0.norm3.bias        |   256    |       [256]        |\n",
      "|  unet.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.weight   |  524288  |    [2048, 256]     |\n",
      "|   unet.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias    |   2048   |       [2048]       |\n",
      "|     unet.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.weight     |  262144  |    [256, 1024]     |\n",
      "|      unet.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias      |   256    |       [256]        |\n",
      "|               unet.up_blocks.1.attentions.1.proj_out.weight                |  262144  |    [1024, 256]     |\n",
      "|                unet.up_blocks.1.attentions.1.proj_out.bias                 |   1024   |       [1024]       |\n",
      "|            unet.up_blocks.1.attentions.1.pos_embed.proj.weight             |  262144  |  [256, 256, 2, 2]  |\n",
      "|             unet.up_blocks.1.attentions.1.pos_embed.proj.bias              |   256    |       [256]        |\n",
      "|               unet.up_blocks.1.attentions.1.norm_out.weight                |   256    |       [256]        |\n",
      "|                unet.up_blocks.1.attentions.1.norm_out.bias                 |   256    |       [256]        |\n",
      "|      unet.up_blocks.1.attentions.2.transformer_blocks.0.norm1.weight       |   256    |       [256]        |\n",
      "|       unet.up_blocks.1.attentions.2.transformer_blocks.0.norm1.bias        |   256    |       [256]        |\n",
      "|    unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_q.weight    |  65536   |     [256, 256]     |\n",
      "|    unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_k.weight    |  32768   |     [256, 128]     |\n",
      "|    unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_v.weight    |  32768   |     [256, 128]     |\n",
      "|  unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.weight  |  65536   |     [256, 256]     |\n",
      "|   unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.bias   |   256    |       [256]        |\n",
      "|      unet.up_blocks.1.attentions.2.transformer_blocks.0.norm2.weight       |   256    |       [256]        |\n",
      "|       unet.up_blocks.1.attentions.2.transformer_blocks.0.norm2.bias        |   256    |       [256]        |\n",
      "|    unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_q.weight    |  65536   |     [256, 256]     |\n",
      "|    unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_k.weight    |  32768   |     [256, 128]     |\n",
      "|    unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_v.weight    |  32768   |     [256, 128]     |\n",
      "|  unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.weight  |  65536   |     [256, 256]     |\n",
      "|   unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.bias   |   256    |       [256]        |\n",
      "|      unet.up_blocks.1.attentions.2.transformer_blocks.0.norm3.weight       |   256    |       [256]        |\n",
      "|       unet.up_blocks.1.attentions.2.transformer_blocks.0.norm3.bias        |   256    |       [256]        |\n",
      "|  unet.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.weight   |  524288  |    [2048, 256]     |\n",
      "|   unet.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.bias    |   2048   |       [2048]       |\n",
      "|     unet.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.weight     |  262144  |    [256, 1024]     |\n",
      "|      unet.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.bias      |   256    |       [256]        |\n",
      "|               unet.up_blocks.1.attentions.2.proj_out.weight                |  262144  |    [1024, 256]     |\n",
      "|                unet.up_blocks.1.attentions.2.proj_out.bias                 |   1024   |       [1024]       |\n",
      "|            unet.up_blocks.1.attentions.2.pos_embed.proj.weight             |  262144  |  [256, 256, 2, 2]  |\n",
      "|             unet.up_blocks.1.attentions.2.pos_embed.proj.bias              |   256    |       [256]        |\n",
      "|               unet.up_blocks.1.attentions.2.norm_out.weight                |   256    |       [256]        |\n",
      "|                unet.up_blocks.1.attentions.2.norm_out.bias                 |   256    |       [256]        |\n",
      "|                  unet.up_blocks.1.resnets.0.norm1.weight                   |   1280   |       [1280]       |\n",
      "|                   unet.up_blocks.1.resnets.0.norm1.bias                    |   1280   |       [1280]       |\n",
      "|                  unet.up_blocks.1.resnets.0.conv1.weight                   | 2949120  | [256, 1280, 3, 3]  |\n",
      "|                   unet.up_blocks.1.resnets.0.conv1.bias                    |   256    |       [256]        |\n",
      "|              unet.up_blocks.1.resnets.0.time_emb_proj.weight               |  65536   |     [256, 256]     |\n",
      "|               unet.up_blocks.1.resnets.0.time_emb_proj.bias                |   256    |       [256]        |\n",
      "|                  unet.up_blocks.1.resnets.0.norm2.weight                   |   256    |       [256]        |\n",
      "|                   unet.up_blocks.1.resnets.0.norm2.bias                    |   256    |       [256]        |\n",
      "|                  unet.up_blocks.1.resnets.0.conv2.weight                   |  589824  |  [256, 256, 3, 3]  |\n",
      "|                   unet.up_blocks.1.resnets.0.conv2.bias                    |   256    |       [256]        |\n",
      "|              unet.up_blocks.1.resnets.0.conv_shortcut.weight               |  327680  | [256, 1280, 1, 1]  |\n",
      "|               unet.up_blocks.1.resnets.0.conv_shortcut.bias                |   256    |       [256]        |\n",
      "|                  unet.up_blocks.1.resnets.1.norm1.weight                   |   512    |       [512]        |\n",
      "|                   unet.up_blocks.1.resnets.1.norm1.bias                    |   512    |       [512]        |\n",
      "|                  unet.up_blocks.1.resnets.1.conv1.weight                   | 1179648  |  [256, 512, 3, 3]  |\n",
      "|                   unet.up_blocks.1.resnets.1.conv1.bias                    |   256    |       [256]        |\n",
      "|              unet.up_blocks.1.resnets.1.time_emb_proj.weight               |  65536   |     [256, 256]     |\n",
      "|               unet.up_blocks.1.resnets.1.time_emb_proj.bias                |   256    |       [256]        |\n",
      "|                  unet.up_blocks.1.resnets.1.norm2.weight                   |   256    |       [256]        |\n",
      "|                   unet.up_blocks.1.resnets.1.norm2.bias                    |   256    |       [256]        |\n",
      "|                  unet.up_blocks.1.resnets.1.conv2.weight                   |  589824  |  [256, 256, 3, 3]  |\n",
      "|                   unet.up_blocks.1.resnets.1.conv2.bias                    |   256    |       [256]        |\n",
      "|              unet.up_blocks.1.resnets.1.conv_shortcut.weight               |  131072  |  [256, 512, 1, 1]  |\n",
      "|               unet.up_blocks.1.resnets.1.conv_shortcut.bias                |   256    |       [256]        |\n",
      "|                  unet.up_blocks.1.resnets.2.norm1.weight                   |   320    |       [320]        |\n",
      "|                   unet.up_blocks.1.resnets.2.norm1.bias                    |   320    |       [320]        |\n",
      "|                  unet.up_blocks.1.resnets.2.conv1.weight                   |  737280  |  [256, 320, 3, 3]  |\n",
      "|                   unet.up_blocks.1.resnets.2.conv1.bias                    |   256    |       [256]        |\n",
      "|              unet.up_blocks.1.resnets.2.time_emb_proj.weight               |  65536   |     [256, 256]     |\n",
      "|               unet.up_blocks.1.resnets.2.time_emb_proj.bias                |   256    |       [256]        |\n",
      "|                  unet.up_blocks.1.resnets.2.norm2.weight                   |   256    |       [256]        |\n",
      "|                   unet.up_blocks.1.resnets.2.norm2.bias                    |   256    |       [256]        |\n",
      "|                  unet.up_blocks.1.resnets.2.conv2.weight                   |  589824  |  [256, 256, 3, 3]  |\n",
      "|                   unet.up_blocks.1.resnets.2.conv2.bias                    |   256    |       [256]        |\n",
      "|              unet.up_blocks.1.resnets.2.conv_shortcut.weight               |  81920   |  [256, 320, 1, 1]  |\n",
      "|               unet.up_blocks.1.resnets.2.conv_shortcut.bias                |   256    |       [256]        |\n",
      "|                 unet.up_blocks.1.upsamplers.0.conv.weight                  |  589824  |  [256, 256, 3, 3]  |\n",
      "|                  unet.up_blocks.1.upsamplers.0.conv.bias                   |   256    |       [256]        |\n",
      "|                  unet.up_blocks.2.resnets.0.norm1.weight                   |   320    |       [320]        |\n",
      "|                   unet.up_blocks.2.resnets.0.norm1.bias                    |   320    |       [320]        |\n",
      "|                  unet.up_blocks.2.resnets.0.conv1.weight                   |  184320  |  [64, 320, 3, 3]   |\n",
      "|                   unet.up_blocks.2.resnets.0.conv1.bias                    |    64    |        [64]        |\n",
      "|              unet.up_blocks.2.resnets.0.time_emb_proj.weight               |  16384   |     [64, 256]      |\n",
      "|               unet.up_blocks.2.resnets.0.time_emb_proj.bias                |    64    |        [64]        |\n",
      "|                  unet.up_blocks.2.resnets.0.norm2.weight                   |    64    |        [64]        |\n",
      "|                   unet.up_blocks.2.resnets.0.norm2.bias                    |    64    |        [64]        |\n",
      "|                  unet.up_blocks.2.resnets.0.conv2.weight                   |  36864   |   [64, 64, 3, 3]   |\n",
      "|                   unet.up_blocks.2.resnets.0.conv2.bias                    |    64    |        [64]        |\n",
      "|              unet.up_blocks.2.resnets.0.conv_shortcut.weight               |  20480   |  [64, 320, 1, 1]   |\n",
      "|               unet.up_blocks.2.resnets.0.conv_shortcut.bias                |    64    |        [64]        |\n",
      "|                  unet.up_blocks.2.resnets.1.norm1.weight                   |   128    |       [128]        |\n",
      "|                   unet.up_blocks.2.resnets.1.norm1.bias                    |   128    |       [128]        |\n",
      "|                  unet.up_blocks.2.resnets.1.conv1.weight                   |  73728   |  [64, 128, 3, 3]   |\n",
      "|                   unet.up_blocks.2.resnets.1.conv1.bias                    |    64    |        [64]        |\n",
      "|              unet.up_blocks.2.resnets.1.time_emb_proj.weight               |  16384   |     [64, 256]      |\n",
      "|               unet.up_blocks.2.resnets.1.time_emb_proj.bias                |    64    |        [64]        |\n",
      "|                  unet.up_blocks.2.resnets.1.norm2.weight                   |    64    |        [64]        |\n",
      "|                   unet.up_blocks.2.resnets.1.norm2.bias                    |    64    |        [64]        |\n",
      "|                  unet.up_blocks.2.resnets.1.conv2.weight                   |  36864   |   [64, 64, 3, 3]   |\n",
      "|                   unet.up_blocks.2.resnets.1.conv2.bias                    |    64    |        [64]        |\n",
      "|              unet.up_blocks.2.resnets.1.conv_shortcut.weight               |   8192   |  [64, 128, 1, 1]   |\n",
      "|               unet.up_blocks.2.resnets.1.conv_shortcut.bias                |    64    |        [64]        |\n",
      "|                  unet.up_blocks.2.resnets.2.norm1.weight                   |   128    |       [128]        |\n",
      "|                   unet.up_blocks.2.resnets.2.norm1.bias                    |   128    |       [128]        |\n",
      "|                  unet.up_blocks.2.resnets.2.conv1.weight                   |  73728   |  [64, 128, 3, 3]   |\n",
      "|                   unet.up_blocks.2.resnets.2.conv1.bias                    |    64    |        [64]        |\n",
      "|              unet.up_blocks.2.resnets.2.time_emb_proj.weight               |  16384   |     [64, 256]      |\n",
      "|               unet.up_blocks.2.resnets.2.time_emb_proj.bias                |    64    |        [64]        |\n",
      "|                  unet.up_blocks.2.resnets.2.norm2.weight                   |    64    |        [64]        |\n",
      "|                   unet.up_blocks.2.resnets.2.norm2.bias                    |    64    |        [64]        |\n",
      "|                  unet.up_blocks.2.resnets.2.conv2.weight                   |  36864   |   [64, 64, 3, 3]   |\n",
      "|                   unet.up_blocks.2.resnets.2.conv2.bias                    |    64    |        [64]        |\n",
      "|              unet.up_blocks.2.resnets.2.conv_shortcut.weight               |   8192   |  [64, 128, 1, 1]   |\n",
      "|               unet.up_blocks.2.resnets.2.conv_shortcut.bias                |    64    |        [64]        |\n",
      "|       unet.mid_block.attentions.0.transformer_blocks.0.norm1.weight        |   1024   |       [1024]       |\n",
      "|        unet.mid_block.attentions.0.transformer_blocks.0.norm1.bias         |   1024   |       [1024]       |\n",
      "|     unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_q.weight     | 1048576  |    [1024, 1024]    |\n",
      "|     unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_k.weight     |  131072  |    [1024, 128]     |\n",
      "|     unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_v.weight     |  131072  |    [1024, 128]     |\n",
      "|   unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.weight   | 1048576  |    [1024, 1024]    |\n",
      "|    unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.bias    |   1024   |       [1024]       |\n",
      "|       unet.mid_block.attentions.0.transformer_blocks.0.norm2.weight        |   1024   |       [1024]       |\n",
      "|        unet.mid_block.attentions.0.transformer_blocks.0.norm2.bias         |   1024   |       [1024]       |\n",
      "|     unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_q.weight     | 1048576  |    [1024, 1024]    |\n",
      "|     unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_k.weight     |  131072  |    [1024, 128]     |\n",
      "|     unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_v.weight     |  131072  |    [1024, 128]     |\n",
      "|   unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.weight   | 1048576  |    [1024, 1024]    |\n",
      "|    unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.bias    |   1024   |       [1024]       |\n",
      "|       unet.mid_block.attentions.0.transformer_blocks.0.norm3.weight        |   1024   |       [1024]       |\n",
      "|        unet.mid_block.attentions.0.transformer_blocks.0.norm3.bias         |   1024   |       [1024]       |\n",
      "|   unet.mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.weight    | 8388608  |    [8192, 1024]    |\n",
      "|    unet.mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.bias     |   8192   |       [8192]       |\n",
      "|      unet.mid_block.attentions.0.transformer_blocks.0.ff.net.2.weight      | 4194304  |    [1024, 4096]    |\n",
      "|       unet.mid_block.attentions.0.transformer_blocks.0.ff.net.2.bias       |   1024   |       [1024]       |\n",
      "|                unet.mid_block.attentions.0.proj_out.weight                 | 4194304  |    [4096, 1024]    |\n",
      "|                 unet.mid_block.attentions.0.proj_out.bias                  |   4096   |       [4096]       |\n",
      "|             unet.mid_block.attentions.0.pos_embed.proj.weight              | 4194304  | [1024, 1024, 2, 2] |\n",
      "|              unet.mid_block.attentions.0.pos_embed.proj.bias               |   1024   |       [1024]       |\n",
      "|                unet.mid_block.attentions.0.norm_out.weight                 |   1024   |       [1024]       |\n",
      "|                 unet.mid_block.attentions.0.norm_out.bias                  |   1024   |       [1024]       |\n",
      "|                   unet.mid_block.resnets.0.norm1.weight                    |   1024   |       [1024]       |\n",
      "|                    unet.mid_block.resnets.0.norm1.bias                     |   1024   |       [1024]       |\n",
      "|                   unet.mid_block.resnets.0.conv1.weight                    | 9437184  | [1024, 1024, 3, 3] |\n",
      "|                    unet.mid_block.resnets.0.conv1.bias                     |   1024   |       [1024]       |\n",
      "|               unet.mid_block.resnets.0.time_emb_proj.weight                |  262144  |    [1024, 256]     |\n",
      "|                unet.mid_block.resnets.0.time_emb_proj.bias                 |   1024   |       [1024]       |\n",
      "|                   unet.mid_block.resnets.0.norm2.weight                    |   1024   |       [1024]       |\n",
      "|                    unet.mid_block.resnets.0.norm2.bias                     |   1024   |       [1024]       |\n",
      "|                   unet.mid_block.resnets.0.conv2.weight                    | 9437184  | [1024, 1024, 3, 3] |\n",
      "|                    unet.mid_block.resnets.0.conv2.bias                     |   1024   |       [1024]       |\n",
      "|                   unet.mid_block.resnets.1.norm1.weight                    |   1024   |       [1024]       |\n",
      "|                    unet.mid_block.resnets.1.norm1.bias                     |   1024   |       [1024]       |\n",
      "|                   unet.mid_block.resnets.1.conv1.weight                    | 9437184  | [1024, 1024, 3, 3] |\n",
      "|                    unet.mid_block.resnets.1.conv1.bias                     |   1024   |       [1024]       |\n",
      "|               unet.mid_block.resnets.1.time_emb_proj.weight                |  262144  |    [1024, 256]     |\n",
      "|                unet.mid_block.resnets.1.time_emb_proj.bias                 |   1024   |       [1024]       |\n",
      "|                   unet.mid_block.resnets.1.norm2.weight                    |   1024   |       [1024]       |\n",
      "|                    unet.mid_block.resnets.1.norm2.bias                     |   1024   |       [1024]       |\n",
      "|                   unet.mid_block.resnets.1.conv2.weight                    | 9437184  | [1024, 1024, 3, 3] |\n",
      "|                    unet.mid_block.resnets.1.conv2.bias                     |   1024   |       [1024]       |\n",
      "|                         unet.conv_norm_out.weight                          |    64    |        [64]        |\n",
      "|                          unet.conv_norm_out.bias                           |    64    |        [64]        |\n",
      "|                            unet.conv_out.weight                            |   1728   |   [3, 64, 3, 3]    |\n",
      "|                             unet.conv_out.bias                             |    3     |        [3]         |\n",
      "+----------------------------------------------------------------------------+----------+--------------------+\n",
      "Total Params: 337742723\n"
     ]
    }
   ],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "337742723"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.unet.down_blocks[2].attentions[0].is_input_patches\n",
    "#model.unet.config.attention_head_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "519062147"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "67108864*3 + 4194304*2 + 1024*3 + 256*2 + 309343363"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.unet.down_blocks[2].attentions[0].config.sample_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
