{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../scripts/diffuser_icons\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import json, random\n",
    "import numpy as np\n",
    "from dataset import draw_icon\n",
    "import torch\n",
    "from torch.nn import Conv2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw Icons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../data/matplotlib/unicode.jsonl\", \"r\", encoding=\"unicode-escape\") as f: lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "icon: ♉ (taurus)\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAAgACADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD3+iquo2EGqafPY3JkEMy7WMUjRsB7MpBB+lecaD4iufAXiK58LeL9UkbTpD5uj6revw6d4pJDxuXjk/yK0Aeo0V5v4t8cHWJIPC3gbUIbvWb5tst3auJI7GH+OQsOAewHX8cZ7XQNDg8PaTHYQ3F1c7Tuea6maWSRj1Yknv6DigDTry1dItviB8UNeh18G40zw95ENpp7MRG0kiFmkcDqeMD2+lepVw/iLwZqx8SHxP4S1SHT9XkiEN1DdIXt7tR93fjkEdMjsO3cA5/xZ4e074f6zoXibwzAunvNqMNheWkJIiuYpCcjb03DGRj69q9Yrz6w8GeI9Z8QWOseN9Vsrgac/m2enadGywLL2kYt8zEdvT8wfQaAP//Z",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAIAAAD8GO2jAAACr0lEQVR4Ae1VP0h6URROSyrFQdKQwARFaFMDcRIxmiQEJXRrbXOQJh2anBwEV0HEQUFBcCh0iLZSnFRUsCkxaGnqD+Kf7Ptx4XK7T9/jB7V1B/3OOd8533vn3nuebLFYrP3mkv9m8X+1/wQkO/zXIskWbQgZjUaj1+vZbDa73S6MrvIMBoN6vW4wGDwezzcOLhq3Li8vCcPhcDw8PHBRofnx8XF6ekpSzs/POcIaZ8Mcj8e1Ws1oNCJnf39/NBoJOawnGAyCubOzUy6X39/f2RDwEgHCwLOrVCpk+nw+Loc1M5kMOHK5/O7ujvVTvFIAjFQqhWSsm5sbmsCCp6cnjUYDQiQSYf0sFhP4/Px0u93It1gsz8/PbBrw6+ury+UiUWwDF6WmmABI2ACTyYQqZrO5UqnMZjM4IYx3slqt8Gu12na7TcsJgYQAEh4fH4kGym1tbeFtlEolMJZOp2u1WsKirEdaAOxYLEYqcr9nZ2fT6ZQtJ8TSAuFwmNQ9OTnJ5XJoTqFQCIVCMpkM/kAggI4J61KPhECpVEKV9fX1fD5PcwioVqvb29uIJpNJLsSaEgIHBwcoEY/H2RyK0+k0orhik8mEOjkgJtDv95GPLV11Cufz+d7eHji3t7dcXWqKfQ8wv5CMqUdaAcwtXGCn0wknrj0XoqaYAE49eBsbSyYuzSdRnCXq4YCYACYd2N1uF63g0qiJewBMJiN1fgO0WUKAurhKYGezWWEUnqurK0Rx+zA2lhLgFNtkhBOJBEqo1er7+3uuRKfTIfIXFxdciDUlBNDc4+NjaGxubmJkQmY4HDabzWg0Sob54eGh8BvwHwKgvr290Q8WlNh1dHT08vLClhNiiTegCdfX136/X6/XKxSK3d1dr9dbLBbFhwTJleGPfagfx2LH9EfEfl3gC1XZSMs9nUG2AAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=32x32>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# draw a single icon\n",
    "tmp = random.choice(lines)\n",
    "tmp = json.loads(tmp)\n",
    "unicode = (tmp[0], tmp[2])\n",
    "print(f\"icon: {tmp[0]} ({tmp[1]})\")\n",
    "im = draw_icon(unicode, unicode2=None, canvas_size=32, icon_size=32, fontsize=28)\n",
    "im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "icon1: ♯ (sharp_music)\n",
      "icon2: ❝ (quotation_open)\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCABAACADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD3+iisPxL4nsPC0djcaldQW1rPO0ckkrfdURO+QByTlVH/AAKgDcorjPAniZvEt1rc9vY3aaZ9qDW13cfJ5uUUFVQ8gDAOT/ers6ACvP8Ax/d+FdH1/wAP6trUUM1/HOyRxBDNK0flS4CR85/eFOcdcc16BWVqGiw3etaVqiwW/wBps5mLzMv7zyzFIu1TjP3nBxwOPpQBgeA9V1XWLrXLq60h9NsXu90CXLYuCxRc70HC8BeOvJrtKht7SG1adoU2meQyyck5bAGfyAqagAqveX1pp8Hn3t1BbRZx5k0gRc/U1Yr5bk+NN7a+OL3UrvRrHUEFyyRfaNxkghBwEiOdqHAyTtJJPPYAA+nLi/s7S1F1c3cENucESySBUOenJOKmR1kRXRgyMAVZTkEeor5ZHxuux4wuNZn0Wyv4WmIgF1uMlvB2SM52occk4JJPoAK+ntMv4dV0qz1G23eRdwJPHuGDtZQwz+BoAtV5Zq/wF8L6v4ll1dp72CKeUyzWkTKEZicnBxlQT29+MV6nRQB5TqnwA8Kajrx1GOW7tLd23yWUBURk9wpxlQfT8sV6jbW8NpaxW1vGscEKCONF6KoGAB+FS0UAf//Z",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAABACAIAAAD07OL5AAADVklEQVR4Ae1XTSi8QRy2vvMRIh+7KUJ2LyJKOWFbS05OborDWifJwYFyUXJyEZKD8nnaPQh7Wi4ipdRqlwPyUbuh9iDy2Xr8p97WvDPvzH+zt3cP28zz/n7PM/P85p2Z1xCJRBLi+UuMJ/kPty4gdFi3KC4WBYPB3d1dITUJiKUG19fXW1tbcRSQpP6/GUxNTal5ZbYZWYso0wOBgN1uPzo6UqvSCEYh8wMdCdvb27NarYQFAsLcBGEECbDZbC6Xq7GxkVBXVFS43W6ZXLHA19fX6upqZmYmoa6trR0ZGRkeHpZhR4xYwOl0EuqWlhaPx4Ocg4MDeYFkkqzx//n52dfXd35+7vV6NcJ4j8QC8/PzSUlJ7e3tPAptXLxMwa6muL+/Hx0dPT09VT+iEclaKcv07u6ut7eXsKAYwnRxkQkFBC4vLx0OR2pqKtjz8vKmp6c/Pj7+RsDn8xUXFycn/xTMZDJBRn4ViWswMTFRU1MTCoVAPTs7e3FxoVhE283qi1fR2dlZdXV1SkrK8fEx/lkkWph4BuPj436/32g0xsD+oyysklJkJRK1fXp6UrraDfEM1PNHtbOystQ4E5EVSEyUjaRkDJggBf1tN8ZxyQ9CFxB6pVukW0QcCIfDc3Nz3d3dzc3Ny8vLii2vr6/r6+s4f9ra2hYWFhT8V0N7L8RT3Cry8/O7uro2Nzdvb29xiyEpuI0VFha2trZubGxcXV3xjk/Bdj04OFhSUnJ4eEiNY2xsDMfyzs4Ohau7WgIzMzM44k9OTqi0tbU1bK77+/sUzuxyBR4fH3NyckpLS+vr69PT09EYGhrCOfPy8oI5FRUVNTU1ZWRkoNHf3//w8MBkB8gVgPW/avWvU1dXt7i4qMarqqowIKYGV6Cnp0dNBCQtLY2JDwwMMAW4exFmzSR6e3tj4vhcYOJcgdzcXGYCD8S7wnzEFcA7xUwoKChg4mazmYlza/D8/FxeXk7lGAwGfA02NDRQOLorKyvMGnAFEI1PycrKSoULX1FLS0vAb25uLBaLguMWMzk5yWQHKLhVvL+/43WFEnaFzs5OrHrCiw1je3sbt8rs7OyOjo6ysjJFj2oIBKjoGLrcIsfAxUzRBZi2RIO6RdFuMNu6RUxbosG4W/QNxmEowAbHV9MAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=32x64>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# draw a two icons \n",
    "tmp1, tmp2 = random.sample(lines, 2)\n",
    "tmp1, tmp2 = json.loads(tmp1), json.loads(tmp2)\n",
    "unicode1 = (tmp1[0], tmp1[2])\n",
    "unicode2 = (tmp2[0], tmp2[2])\n",
    "print(f\"icon1: {tmp1[0]} ({tmp1[1]})\\nicon2: {tmp2[0]} ({tmp2[1]})\")\n",
    "im = draw_icon(unicode1, unicode2, canvas_size=(64, 32), icon_size=32, fontsize=28)\n",
    "im"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Generated Icons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 icons in total, filter shape = torch.Size([200, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "### Create all filters\n",
    "icon_size, font_size = 32, 28\n",
    "#icon_size, font_size = 128, 128, 120\n",
    "GTHS = []\n",
    "with open(\"../../data/nouns/all_nouns.txt\", \"r\") as f: nouns = [x.strip() for x in f.readlines()]\n",
    "with open(\"../../data/matplotlib/unicode.jsonl\", \"r\", encoding=\"unicode-escape\") as f: lines = f.readlines()\n",
    "icons_names = [json.loads(x)[1] for x in lines]\n",
    "nouns_to_names = {n:m for n, m in zip(nouns, icons_names)}\n",
    "for l in lines:\n",
    "    im = Image.new(\"RGB\", (icon_size, icon_size), (255,255,255))\n",
    "    draw = ImageDraw.Draw(im)\n",
    "    unicode_text, _, font = json.loads(l)\n",
    "    unicode_font = ImageFont.truetype(f\"fonts/{font}.ttf\", font_size)\n",
    "    _, _, w, h = draw.textbbox((0, 0), unicode_text, font=unicode_font)\n",
    "    draw.text(((icon_size-w)/2,(icon_size-h)/2), unicode_text, font=unicode_font, fill=\"black\")\n",
    "    y = torch.Tensor(np.asarray(im)/255 - 0.5).permute(2, 0, 1)\n",
    "    GTHS.append(y)\n",
    "GTHS = torch.stack(GTHS)\n",
    "print(f\"{len(lines)} icons in total, filter shape = {GTHS.shape}\")\n",
    "\n",
    "filter = Conv2d(\n",
    "    in_channels=3,\n",
    "    out_channels=1,\n",
    "    kernel_size=(icon_size, icon_size),\n",
    "    bias=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated 20 samples\n",
      "Acc = 1.0 (20/20)\n"
     ]
    }
   ],
   "source": [
    "### Eval single object\n",
    "\n",
    "img_path = \"output/1213_154255/samples/4999_30000.png\" # a batch of generated images is saved to the same file\n",
    "\n",
    "im = Image.open(img_path).convert(\"RGB\")\n",
    "im_array = np.array(im, dtype=np.float64)\n",
    "h, w, d = tuple(im_array.shape)\n",
    "\n",
    "# Process the image collage and recover the batch_size dimension\n",
    "num_rows, num_cols = int(h // icon_size), int(w // icon_size)\n",
    "bs = num_rows * num_cols\n",
    "pixels = np.reshape(im_array, (num_rows, icon_size, w, d))\n",
    "pixels = np.reshape(pixels, (num_rows, icon_size, num_cols, icon_size, d))\n",
    "pixels = np.transpose(pixels, (0, 2, 1, 3, 4))\n",
    "pixels = np.reshape(pixels, (bs, icon_size, icon_size, d))\n",
    "\n",
    "# Read the text prompts\n",
    "with open(img_path.replace(\".png\", \".txt\"), \"r\") as txt:\n",
    "    input_texts = []\n",
    "    for l in txt.readlines():\n",
    "        if \"=========\" in l: break\n",
    "        input_texts.append(l.strip())   \n",
    "\n",
    "em, count = 0, 0           \n",
    "for m, sentence in zip(pixels, input_texts):\n",
    "    x = torch.Tensor(np.asarray(m)/255 - 0.5).permute(2, 0, 1)\n",
    "    filter.weight.data = x.unsqueeze(0)\n",
    "    output = filter(GTHS).squeeze()\n",
    "    em += int(nouns[output.argmax().item()] == sentence.replace(\".\", \"\").split()[-1])\n",
    "    count += 1\n",
    "\n",
    "\n",
    "print(f\"Evaluated {count} samples\")\n",
    "print(f\"Acc = {em/count} ({em}/{count})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated 20 samples\n",
      "Acc = 0.65 (13/20)\n"
     ]
    }
   ],
   "source": [
    "## Eval two objects\n",
    "\n",
    "img_path = \"output/0213_235617/samples/599_58802.png\" # a batch of generated images is saved to the same file\n",
    "\n",
    "im = Image.open(img_path).convert(\"RGB\")\n",
    "im_array = np.array(im, dtype=np.float64)\n",
    "h, w, d = tuple(im_array.shape)\n",
    "\n",
    "# Process the image collage and recover the batch_size dimension\n",
    "image_height, image_width, icon_size = 64, 32, 32\n",
    "num_rows, num_cols = int(h // image_height), int(w // image_width)\n",
    "bs = num_rows * num_cols\n",
    "pixels = np.reshape(im_array, (num_rows, image_height, w, d))\n",
    "pixels = np.reshape(pixels, (num_rows, image_height, num_cols, image_width, d))\n",
    "pixels = np.transpose(pixels, (0, 2, 1, 3, 4))\n",
    "pixels = np.reshape(pixels, (bs, image_height, image_width, d))\n",
    "\n",
    "# Read the text prompts\n",
    "with open(img_path.replace(\".png\", \".txt\"), \"r\") as txt:\n",
    "    l = \"\".join(txt.readlines()).split(\"============\")[0].strip().split(\"\\n\")\n",
    "    input_texts = [x.strip() for x in l]\n",
    "\n",
    "em, count = 0, 0\n",
    "for m, sentence in zip(pixels, input_texts):\n",
    "    upper_m, lower_m = m[:icon_size,:], m[icon_size:,:] \n",
    "    objects = [w for w in sentence.replace(\".\", \"\").split() if w in nouns]\n",
    "    if \"bottom\" in sentence: objects = objects[::-1]\n",
    "    upper_o, lower_o = objects # gth objects\n",
    "\n",
    "    x = torch.Tensor(np.asarray(upper_m)/255 - 0.5).permute(2, 0, 1)\n",
    "    filter.weight.data = x.unsqueeze(0)\n",
    "    output = filter(GTHS).squeeze()\n",
    "    upper_gen = nouns[output.argmax().item()]\n",
    "    upper_em = int(upper_gen == upper_o)\n",
    "\n",
    "    x = torch.Tensor(np.asarray(lower_m)/255 - 0.5).permute(2, 0, 1)\n",
    "    filter.weight.data = x.unsqueeze(0)\n",
    "    output = filter(GTHS).squeeze()\n",
    "    lower_gen = nouns[output.argmax().item()]\n",
    "    lower_em = int(lower_gen == lower_o)\n",
    "    \n",
    "    em += upper_em * lower_em # em = 1 if both objects are correct\n",
    "    count += 1\n",
    "\n",
    "print(f\"Evaluated {count} samples\")\n",
    "print(f\"Acc = {em/count} ({em}/{count})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
