{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, random\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image\n",
    "from typing import Any, Dict, Optional, Tuple, Union, List\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "from transformers import ViTForImageClassification\n",
    "import pytz\n",
    "from datetime import datetime\n",
    "timezone = pytz.timezone('America/New_York') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"<largefiles_dir>/skewed_relations_T2I/\", # [Important]: update this to the correct path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3924994/3253253806.py:6: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"accuracy\")\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTImageProcessor\n",
    "model_name_or_path = 'google/vit-base-patch16-224-in21k'\n",
    "processor = ViTImageProcessor.from_pretrained(model_name_or_path)\n",
    "\n",
    "from datasets import load_metric\n",
    "metric = load_metric(\"accuracy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc(output, gth, verbose=False):\n",
    "    output, gth = output.detach().cpu(), gth.detach().cpu()\n",
    "    if verbose:\n",
    "        print(\"pred = \", np.argmax(output, axis=1))\n",
    "        print(\"gth  = \", gth)\n",
    "    return metric.compute(predictions=np.argmax(output, axis=1), references=gth)['accuracy'], np.argmax(output, axis=1)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    inputs = {}\n",
    "    inputs['pixel_values'] = processor([x['image'] for x in batch], return_tensors='pt')['pixel_values']\n",
    "    inputs['label'] = torch.LongTensor([x['label'] for x in batch])\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 classes: List[str], # used to define class_ids, order matters\n",
    "                 imdir: str,\n",
    "                 data: List,\n",
    "                 imsize = 64,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.imdir = imdir\n",
    "        self.data = data ## should be a list of tuples, e.g. (text, image, (mug, cup, left of))\n",
    "        self.imsize = imsize\n",
    "\n",
    "        #self.classes = classes\n",
    "        self.classes = [\"empty\"]+classes\n",
    "        self.n2i = {n:i for i, n in enumerate(self.classes)}\n",
    "\n",
    "    def __len__(self): return len(self.data)*2\n",
    "    \n",
    "    def __getitem__(self, i): \n",
    "        text, image_path, tuple = self.data[i//2]\n",
    "\n",
    "        # center crop to a square, then resize\n",
    "        image = Image.open(os.path.join(self.imdir, image_path))\n",
    "        width, height = image.size\n",
    "        new_dimension = min(image.size)\n",
    "        left = (width - new_dimension)/2\n",
    "        top = (height - new_dimension)/2\n",
    "        right = (width + new_dimension)/2\n",
    "        bottom = (height + new_dimension)/2\n",
    "        image = image.crop((left, top, right, bottom)).resize((self.imsize, self.imsize))\n",
    "        \n",
    "        H, W = self.imsize, self.imsize\n",
    "\n",
    "        if tuple[-1] in ['left of', 'right of']:\n",
    "            crop1 = image.crop((0, H//4, W//2, 3*H//4))\n",
    "            crop2 = image.crop((W//2, H//4, W, 3*H//4))\n",
    "\n",
    "            crop3 = image.crop((W//4, 0, 3*W//4, H//2))\n",
    "            crop4 = image.crop((W//4, H//2, 3*W//4, H))\n",
    "        else:\n",
    "            crop1 = image.crop((W//4, 0, 3*W//4, H//2))\n",
    "            crop2 = image.crop((W//4, H//2, 3*W//4, H))\n",
    "\n",
    "            crop3 = image.crop((0, H//4, W//2, 3*H//4))\n",
    "            crop4 = image.crop((W//2, H//4, W, 3*H//4))\n",
    "\n",
    "        f1, f2 = self.n2i[tuple[0]], self.n2i[tuple[1]]\n",
    "        if tuple[-1] in ['right of', 'in-front of']:\n",
    "            f1, f2 = f2, f1\n",
    "        f3, f4 = 0, 0\n",
    "        image = [crop1, crop2, crop3, crop4][i%4]\n",
    "        label = [f1, f2, f3, f4][i%4]\n",
    "\n",
    "        \"\"\"\n",
    "        dataset return\n",
    "        {\n",
    "            \"image\": pilimage,\n",
    "            \"labels\": int\n",
    "        }\n",
    "        \"\"\"\n",
    "        # res = {\n",
    "        #     \"sentence\": text,\n",
    "        #     'image': crop2 if i%2 else crop1,\n",
    "        #     'label': f2 if i%2 else f1,\n",
    "        # }\n",
    "        res = {\n",
    "            \"sentence\": text,\n",
    "            'image': image,\n",
    "            'label': label,\n",
    "        }\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['book', 'bowl', 'can', 'cap', 'cup', 'mug', 'plate', 'candle', 'flower', 'fork', 'headphones', 'knife', 'scissors', 'spoon', 'tape']\n",
      "554 training examples, 62 testing examples\n"
     ]
    }
   ],
   "source": [
    "J = json.load(open(\"../data/aggregated/whatsup_vlm_b.json\", \"r\"))\n",
    "annotations = []\n",
    "for a in J:\n",
    "    if a[-1][0] in [\"sunglasses\", \"remote\", \"phone\"] or a[-1][1] in [\"sunglasses\", \"remote\", \"phone\"]: continue\n",
    "    else: annotations.append(a)\n",
    "occurrences = [a[-1][0] for a in annotations] + [a[-1][1] for a in annotations]\n",
    "c = Counter(occurrences)\n",
    "\n",
    "classes = sorted(c.keys(), key=lambda x: (-c[x], x))\n",
    "\n",
    "print(classes)\n",
    "# Each whatsup image contains two objects. \n",
    "# We will resize the image to 64x64, and take two 32x32 single-obj crops\n",
    "D = dataset(\n",
    "    classes, \n",
    "    imdir = os.path.join(PATH, \"data\"),\n",
    "    data = annotations,\n",
    "    imsize=64 \n",
    ")\n",
    "\n",
    "train_ratio = 0.9\n",
    "train_num = int(train_ratio*len(D))\n",
    "val_num = len(D) - train_num\n",
    "train_data, val_data = random_split(D, [train_num, val_num])\n",
    "print(f\"{len(train_data)} training examples, {len(val_data)} testing examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, dataloader, optimizer, criterion, device):\n",
    "    net.train()\n",
    "    running_loss, running_acc = [], []\n",
    "    for batch in tqdm(dataloader):\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        bs = len(batch['pixel_values'])\n",
    "        outputs = net(batch['pixel_values'].to(device))['logits']# (bs, outputs_per_sample*num_classes)\n",
    "\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        acc, _ = get_acc(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss.append(loss.item())\n",
    "        #print(acc)\n",
    "        running_acc.append(acc)\n",
    "    return np.mean(running_loss), np.mean(running_acc)\n",
    "\n",
    "def val(net, dataloader, criterion, device, verbose):\n",
    "    net.eval()\n",
    "    epoch_loss, epoch_acc = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            bs = len(batch['pixel_values'])\n",
    "            outputs = net(batch['pixel_values'].to(device))['logits'] # (bs, outputs_per_sample*num_classes)\n",
    "\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            acc, _ = get_acc(outputs, labels, verbose=verbose)\n",
    "\n",
    "            # print statistics\n",
    "            epoch_loss.append(loss.item())\n",
    "            epoch_acc.append(acc)\n",
    "        \n",
    "    return np.mean(epoch_loss), np.mean(epoch_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "labels = D.classes\n",
    "\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    num_labels=len(labels),\n",
    "    id2label={str(i): c for i, c in enumerate(labels)},\n",
    "    label2id={c: str(i) for i, c in enumerate(labels)}\n",
    ")\n",
    "device = \"cuda\"\n",
    "model.to(device)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['empty', 'book', 'bowl', 'can', 'cap', 'cup', 'mug', 'plate', 'candle', 'flower', 'fork', 'headphones', 'knife', 'scissors', 'spoon', 'tape']\n"
     ]
    }
   ],
   "source": [
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=16\n",
    "trainloader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, num_workers=2)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-4) #, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pilimage image size from dataset.__getitem__():  (32, 32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image tensor size after processor torch.Size([16, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "print(\"pilimage image size from dataset.__getitem__(): \", train_data[0]['image'].size)\n",
    "batch = next(iter(trainloader))\n",
    "print(\"image tensor size after processor\", batch['pixel_values'].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_classes =  16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:08<00:00,  4.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01\n",
      "\tTrain Loss: 1.434 | Train Acc: 66.14%\n",
      "\t Val. Loss: 0.672 |  Val. Acc: 93.75%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:06<00:00,  5.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02\n",
      "\tTrain Loss: 0.462 | Train Acc: 96.96%\n",
      "\t Val. Loss: 0.374 |  Val. Acc: 98.44%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|â–ˆâ–ˆâ–Œ       | 9/35 [00:02<00:06,  3.95it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m date \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow(timezone)\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epc \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m---> 10\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     val_loss, val_acc \u001b[38;5;241m=\u001b[39m val(model, testloader, criterion, device, verbose)\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m val_acc \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m best_val_acc:\n",
      "Cell \u001b[0;32mIn[7], line 21\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(net, dataloader, optimizer, criterion, device)\u001b[0m\n\u001b[1;32m     18\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# print statistics\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m running_loss\u001b[38;5;241m.\u001b[39mappend(\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#print(acc)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m running_acc\u001b[38;5;241m.\u001b[39mappend(acc)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf')\n",
    "best_val_acc = 0.0\n",
    "verbose=False\n",
    "print(\"num_classes = \", len(labels))\n",
    "\n",
    "save_to_dir = os.path.join(PATH, \"autoeval\")\n",
    "date = datetime.now(timezone).strftime(\"%m%d_%H%M%S\")\n",
    "for epc in range(10):\n",
    "\n",
    "    train_loss, train_acc = train(model, trainloader, optimizer, criterion, device)\n",
    "    val_loss, val_acc = val(model, testloader, criterion, device, verbose)\n",
    "\n",
    "    if val_acc >= best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), os.path.join(save_to_dir, \"{}_{}.pt\".format(model_name_or_path.split(\"/\")[-1], date)))\n",
    "    \n",
    "    print(f'Epoch: {epc+1:02}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {val_loss:.3f} |  Val. Acc: {val_acc*100:.2f}%')\n",
    "    \n",
    "print(\"Training: finish\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Finetuned ViT on Generated Samples (singleobj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#18 objs in original whatsup dataset (plus 'empty')\n",
    "#classes = ['empty', 'mug', 'plate', 'book', 'bowl', 'can', 'cap', 'cup', 'remote', 'sunglasses', \n",
    "#          'tape', 'candle', 'flower', 'fork', 'headphones', 'scissors', 'spoon', 'knife', 'phone']\n",
    "\n",
    "#15 objs (plus 'empty') after removing sunglasses, remote, phone\n",
    "classes = ['empty', 'book', 'bowl', 'can', 'cap', 'cup', 'mug', 'plate', 'candle', 'flower', 'fork', 'headphones', 'knife', 'scissors', 'spoon', 'tape']\n",
    "\n",
    "n2i = {n:i for i, n in enumerate(classes)}\n",
    "device = \"cuda\"\n",
    "ckpt_dir = os.path.join(PATH, \"autoeval/vit-base-patch16-224-in21k_0311_211459.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 17\n",
      "34 34\n",
      "51 51\n",
      "68 68\n",
      "85 85\n",
      "102 102\n",
      "119 119\n",
      "136 136\n",
      "153 153\n",
      "170 170\n",
      "187 187\n",
      "resolution of generated images:  (32, 32)\n"
     ]
    }
   ],
   "source": [
    "handle = \"0304_002415\" #\"0514_083120\"\n",
    "whichset=\"test\"\n",
    "output_folder=\"output\" #output_rbt\"\n",
    "sample_dir = f\"../scripts/diffuser_real/{output_folder}/{handle}/samples\"\n",
    "\n",
    "# Load samples saved during training (from epoch 0 to completion)\n",
    "\n",
    "imsize = 32 #128\n",
    "\n",
    "pilimages, gth_captions = [], []\n",
    "for f in os.listdir(sample_dir):\n",
    "    if \".txt\" in f: continue\n",
    "    im = Image.open(f\"{sample_dir}/{f}\")\n",
    "    W, H = im.size\n",
    "    nrows, ncols = H//imsize, W//imsize\n",
    "    captions_file = f.replace(\".png\", \".txt\")\n",
    "    with open(f\"{sample_dir}/{captions_file}\", \"r\") as captions:\n",
    "        tmp_captions = [x.strip() for x in captions.readlines()]\n",
    "    tmp_pilimages = []\n",
    "    for r in range(nrows):\n",
    "        for c in range(ncols):\n",
    "            left, top = c*imsize, r*imsize\n",
    "            right, bottom = left+imsize, top+imsize\n",
    "            pilimage = im.crop((left, top, right, bottom))\n",
    "            if np.sum(pilimage) == 255*3*imsize*imsize: continue # skip placeholders which are purely white images\n",
    "            tmp_pilimages.append(pilimage)\n",
    "    for x, pilimage in zip(tmp_captions, tmp_pilimages):\n",
    "        if all([k not in x.split() for k in [\"sunglasses\", \"remote\", \"phone\"]]):\n",
    "            gth_captions.append(x)\n",
    "            pilimages.append(pilimage)\n",
    "    print(len(pilimages), len(gth_captions))\n",
    "print(\"resolution of generated images: \", pilimages[0].size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = ViTForImageClassification.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    num_labels=len(classes),\n",
    "    id2label={str(i): c for i, c in enumerate(classes)},\n",
    "    label2id={c: str(i) for i, c in enumerate(classes)}\n",
    ")\n",
    "\n",
    "state_dict = torch.load(ckpt_dir, map_location=device)\n",
    "model.load_state_dict(state_dict)\n",
    "model.to(device)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACC = []\n",
    "gth_labels, vit_pred = [], []\n",
    "eval_batch_size = 18\n",
    "outer_batch = []\n",
    "dataiter = iter(zip(pilimages, gth_captions))\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        image, text = next(dataiter)\n",
    "        if any([k == text.split()[-1] for k in [\"sunglasses\", \"remote\", \"phone\"]]): continue\n",
    "        outer_batch.append({\n",
    "            \"image\": image,\n",
    "            \"label\": n2i[text.split()[-1]]\n",
    "        })\n",
    "        if len(outer_batch) == eval_batch_size:\n",
    "            input_batch = collate_fn(outer_batch)\n",
    "            labels = input_batch['label'].to(device)\n",
    "            outputs = model(input_batch['pixel_values'].to(device))['logits']\n",
    "\n",
    "            for b in range(eval_batch_size):\n",
    "                acc, pred = get_acc(outputs[b:b+1], labels[b:b+1], False)\n",
    "                ACC.append(acc)\n",
    "                vit_pred.append(pred.item())\n",
    "                gth_labels.append(labels[b])\n",
    "            outer_batch = []\n",
    "    except StopIteration:\n",
    "        if len(outer_batch):\n",
    "            input_batch = collate_fn(outer_batch)\n",
    "            labels = input_batch['label'].to(device)\n",
    "            outputs = model(input_batch['pixel_values'].to(device))['logits']\n",
    "            for b in range(len(outer_batch)):\n",
    "                acc, pred = get_acc(outputs[b:b+1], labels[b:b+1], False)\n",
    "                ACC.append(acc)\n",
    "                vit_pred.append(pred.item())\n",
    "                gth_labels.append(labels[b])\n",
    "        break\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187\n",
      "ACC autoevaluated by ViT:  0.877\n"
     ]
    }
   ],
   "source": [
    "print(len(ACC))\n",
    "print(\"ACC autoevaluated by ViT: \", round(np.mean(ACC), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text prompt: an image of a spoon\n",
      "Score judged by ViT:  0.0\n",
      "Generated objs judged by ViT: <knife>\n",
      "i =  132\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAAgACADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwDRs75M8sfzrZj1RAn3uvvXmEWrspzurqNSH9mWWly+cXa8tRcHPQZJwB+ArWxnc6s6qm3G7j61Wk1RCT8x/OuMbVvl5aoDqvP3qVho86j1IgcNXa65qEqeFPCkjvkPaShT7CZuPyxXlSysO9aM+uXlzp1lYzTl4LPf5CkfcDHJGfrzWmjFZnSNque9RtqfvXLfbCR1pr3jEYBo0BH/2Q==",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=32x32>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize imperfect generations \n",
    "while True:\n",
    "    i = random.choice(list(range(len(pilimages))))\n",
    "    if ACC[i] < 1:\n",
    "        print(\"Text prompt:\", gth_captions[i])\n",
    "        print(\"Score judged by ViT: \", ACC[i])\n",
    "        f = vit_pred[i]\n",
    "        print(f\"Generated objs judged by ViT: <{classes[f]}>\")\n",
    "        break\n",
    "print(\"i = \", i)\n",
    "pilimages[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Finetuned ViT on Generated Samples (two objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#18 objs in original whatsup dataset:\n",
    "#classes = ['empty', 'mug', 'plate', 'book', 'bowl', 'can', 'cap', 'cup', 'remote', 'sunglasses', \n",
    "#          'tape', 'candle', 'flower', 'fork', 'headphones', 'scissors', 'spoon', 'knife', 'phone']\n",
    "\n",
    "#15 objs after removing sunglasses, remote, phone\n",
    "classes = ['empty', 'book', 'bowl', 'can', 'cap', 'cup', 'mug', 'plate', 'candle', 'flower', 'fork', 'headphones', 'knife', 'scissors', 'spoon', 'tape']\n",
    "n2i = {n:i for i, n in enumerate(classes)}\n",
    "device = \"cuda\"\n",
    "ckpt_dir = os.path.join(PATH, \"autoeval/vit-base-patch16-224-in21k_0311_211459.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_gen_sample(pilimage, gth_caption, whichset):\n",
    "    W, H = pilimage.size\n",
    "    if W==H: \n",
    "        crop1 = pilimage.crop((W//4, 0, 3*W//4, H//2)) # behind\n",
    "        crop2 = pilimage.crop((W//4, H//2, 3*W//4, H)) # front\n",
    "        crop3 = pilimage.crop((0, H//4, W//2, 3*H//4)) # left\n",
    "        crop4 = pilimage.crop((W//2, H//4, W, 3*H//4)) # right\n",
    "    elif W == 2*H:\n",
    "        crop3 = pilimage.crop((0, 0, W//2, H)) # left\n",
    "        crop4 = pilimage.crop((W//2, 0, W, H)) # right\n",
    "    elif H == 2*W:\n",
    "        crop1 = pilimage.crop((0, 0, W, H//2)) # behind\n",
    "        crop2 = pilimage.crop((0, H//2, W, H)) # front\n",
    "        \n",
    "    tmp = gth_caption.split()\n",
    "    if whichset == \"train\":\n",
    "        f1, f2 = tmp[1], tmp[-1]\n",
    "        r = \" \".join(tmp[2:-2])\n",
    "    elif whichset == \"test\":\n",
    "        f1, f2 = tmp[0], tmp[-1]\n",
    "        r = \" \".join(tmp[1:-1])\n",
    "    else: raise ValueError(f\"Invalid whichset: {whichset}\")\n",
    "    T = (f1, f2, r)\n",
    "\n",
    "    if \"left of\" in r: labels, crops = [n2i[f1], n2i[f2]], [crop3, crop4]\n",
    "    elif \"right of\" in r: labels, crops = [n2i[f2], n2i[f1]], [crop3, crop4]\n",
    "    elif \"in-front of\" in r: labels, crops = [n2i[f2], n2i[f1]], [crop1, crop2]\n",
    "    elif \"behind\" in r: labels, crops = [n2i[f1], n2i[f2]], [crop1, crop2]\n",
    "    else: raise ValueError(f\"Invalid relation: {r}\")\n",
    "    \n",
    "    return [{\n",
    "        \"image\": crop,\n",
    "        \"label\": label,\n",
    "    } for crop, label in zip(crops, labels)], T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 20\n",
      "40 40\n",
      "60 60\n",
      "80 80\n",
      "100 100\n"
     ]
    }
   ],
   "source": [
    "handle = \"0304_235058\"\n",
    "whichset=\"train\"\n",
    "sample_dir = f\"../scripts/diffuser_real/output/{handle}/samples\"\n",
    "\n",
    "# Load samples saved during training (from epoch 0 to completion)\n",
    "\n",
    "imsize_h, imsize_w = 32, 64\n",
    "\n",
    "pilimages, gth_captions = [], []\n",
    "\n",
    "for f in os.listdir(sample_dir):\n",
    "    if \".txt\" in f: continue\n",
    "    im = Image.open(f\"{sample_dir}/{f}\")\n",
    "    W, H = im.size\n",
    "    nrows, ncols = H//imsize_h, W//imsize_w\n",
    "    captions_file = f.replace(\".png\", \".txt\")\n",
    "    with open(f\"{sample_dir}/{captions_file}\", \"r\") as captions:\n",
    "        gth_captions.extend([x.strip() for x in captions.readlines()])\n",
    "    for r in range(nrows):\n",
    "        for c in range(ncols):\n",
    "            left, top = c*imsize_w, r*imsize_h\n",
    "            right, bottom = left+imsize_w, top+imsize_h\n",
    "            pilimage = im.crop((left, top, right, bottom))\n",
    "            if np.sum(pilimage) == 255*3*imsize_h*imsize_w: continue # skip placeholders which are purely white images\n",
    "            pilimages.append(pilimage)\n",
    "    print(len(pilimages), len(gth_captions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = ViTForImageClassification.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    num_labels=len(classes),\n",
    "    id2label={str(i): c for i, c in enumerate(classes)},\n",
    "    label2id={c: str(i) for i, c in enumerate(classes)}\n",
    ")\n",
    "\n",
    "state_dict = torch.load(ckpt_dir, map_location=device)\n",
    "model.load_state_dict(state_dict)\n",
    "model.to(device)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACC = []\n",
    "vit_pred = []\n",
    "eval_batch_size = 16\n",
    "outer_batch = []\n",
    "dataiter = iter(zip(pilimages, gth_captions))\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        image, text = next(dataiter)\n",
    "        batch, T = process_gen_sample(image, text, whichset)\n",
    "        outer_batch.extend(batch)\n",
    "        if len(outer_batch) == 2*eval_batch_size:\n",
    "            input_batch = collate_fn(outer_batch)\n",
    "            labels = input_batch['label'].to(device)\n",
    "            outputs = model(input_batch['pixel_values'].to(device))['logits']\n",
    "\n",
    "            for b in range(eval_batch_size):\n",
    "                acc, pred = get_acc(outputs[2*b:2*b+2], labels[2*b:2*b+2], False)\n",
    "                ACC.append(acc)\n",
    "                vit_pred.append(pred)\n",
    "            outer_batch = []\n",
    "    except StopIteration:\n",
    "        if len(outer_batch):\n",
    "            input_batch = collate_fn(outer_batch)\n",
    "            labels = input_batch['label'].to(device)\n",
    "            outputs = model(input_batch['pixel_values'].to(device))['logits']\n",
    "            for b in range(len(outer_batch)//2):\n",
    "                acc, pred = get_acc(outputs[2*b:2*b+2], labels[2*b:2*b+2], False)\n",
    "                ACC.append(acc)\n",
    "                vit_pred.append(pred)\n",
    "        break\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "0.765\n",
      "ACC autoevaluated by ViT:  0.69\n"
     ]
    }
   ],
   "source": [
    "print(len(ACC))\n",
    "print(np.mean(ACC))\n",
    "print(\"ACC autoevaluated by ViT: \", np.mean([a==1 for a in ACC]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text prompt: A can to the left of a fork\n",
      "Score judged by ViT:  0.5\n",
      "Generated objs judged by ViT: <fork> on the left, <fork> on the right\n",
      "i =  28\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAAgAEADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwDp/wDhb9z/ANAyD/v4aP8AhcM466TCfpMf8K8Y/tEetNN/nvSQz2ofGRwOdHQ/Sc//ABNNPxoKnnRV/wDAn/7GvIvJuG0h9Syv2dZPLJJ5JrLkv8HrTVmI9vPxr540Vf8AwJ/+xpv/AAuqQnjRo/xuD/8AE14cL8ZzmrssVzDp0N+y/wCjyttRs9Tz/gaGgPaE+Msj/wDMIi/7/n/CpR8Xpj/zCov+/wAf8K8QhvunNXob4HvTSEzi/t59RSfbiOlZeT61NaXAtr2C4aMSrFIrmNujYOcGkM6bWdcv7OwttAMirBEizTIEAbzWy2CevAIGPWsB71m71HqN7JqWpXN7LnfPI0hyc4yc9aq0LQGXReMDXXS61d3fwzt4XitxBaaj5SyKhEhyhYAnODyW7Vwlb9rrltH4IvtDlikNxLexXUMi42jarKwP4MKbswQyK7Ixmr0V3x1rnY5ccE1chkLHjpTEf//Z",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=64x32>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize imperfect generations \n",
    "while True:\n",
    "    i = random.choice(list(range(len(pilimages))))\n",
    "    if ACC[i] < 1:\n",
    "        print(\"Text prompt:\", gth_captions[i])\n",
    "        print(\"Score judged by ViT: \", ACC[i])\n",
    "        f1, f2 = vit_pred[i]\n",
    "        print(f\"Generated objs judged by ViT: <{classes[f1]}> on the left, <{classes[f2]}> on the right\")\n",
    "        break\n",
    "print(\"i = \", i)\n",
    "pilimages[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
