{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yingshac/workspace/clevr_control/LLaVA/venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter, defaultdict\n",
    "from PIL import Image\n",
    "import json, os, random, math, re, sys\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "import torch\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"/data/yingshac/hf_cache\"\n",
    "verbose = False\n",
    "objects = ['mug', 'cap', 'sunglasses', 'remote', 'fork', 'plate', 'headphones', 'tape', 'candle', 'phone', 'spoon', 'book', 'knife', 'flower', 'bowl', 'cup', 'scissors', 'can']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.00s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlavaForConditionalGeneration(\n",
       "  (vision_tower): CLIPVisionModel(\n",
       "    (vision_model): CLIPVisionTransformer(\n",
       "      (embeddings): CLIPVisionEmbeddings(\n",
       "        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "        (position_embedding): Embedding(577, 1024)\n",
       "      )\n",
       "      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (encoder): CLIPEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-23): 24 x CLIPEncoderLayer(\n",
       "            (self_attn): CLIPAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): CLIPMLP(\n",
       "              (activation_fn): QuickGELUActivation()\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (multi_modal_projector): LlavaMultiModalProjector(\n",
       "    (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "    (act): GELUActivation()\n",
       "    (linear_2): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "  )\n",
       "  (language_model): LlamaForCausalLM(\n",
       "    (model): LlamaModel(\n",
       "      (embed_tokens): Embedding(32064, 4096)\n",
       "      (layers): ModuleList(\n",
       "        (0-31): 32 x LlamaDecoderLayer(\n",
       "          (self_attn): LlamaSdpaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): LlamaRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "            (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm()\n",
       "          (post_attention_layernorm): LlamaRMSNorm()\n",
       "        )\n",
       "      )\n",
       "      (norm): LlamaRMSNorm()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=4096, out_features=32064, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device=\"cuda\"\n",
    "# use llava 1.5 7b model\n",
    "model_path = 'llava-hf/llava-1.5-7b-hf'\n",
    "model = LlavaForConditionalGeneration.from_pretrained(model_path, torch_dtype=torch.float32, ) \n",
    "processor = AutoProcessor.from_pretrained(model_path)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llava_vqa(image, prompt, device, verbose=False):\n",
    "    if verbose: print(prompt)\n",
    "    inputs = processor(text=prompt, images=image, return_tensors=\"pt\")\n",
    "    inputs = {k:v.to(device) for k, v in inputs.items()}\n",
    "    generate_ids = model.generate(**inputs, max_length=300)\n",
    "    generated_text = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "    answer = generated_text.split('\\n')[-1]\n",
    "\n",
    "    if verbose: print(answer)\n",
    "    return int('yes' in answer.lower())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(gth_caption, pilimage, device):\n",
    "    # input pilimage: 64x64\n",
    "    tmp = gth_caption.split()\n",
    "    f1, f2 = tmp[0], tmp[-1]\n",
    "    true_relation = \" \".join(tmp[1:-1])\n",
    "    if true_relation in ['right of', 'left of']:\n",
    "        image1 = pilimage.crop((0, 0, 32, 64))\n",
    "        image2 = pilimage.crop((32, 0, 64, 64))\n",
    "    else:\n",
    "        image1 = pilimage.crop((0, 0, 64, 32))\n",
    "        image2 = pilimage.crop((0, 32, 64, 64))\n",
    "\n",
    "    image1 = torch.from_numpy(np.array(image1)).to(torch.float32).to(device)\n",
    "    image2 = torch.from_numpy(np.array(image2)).to(torch.float32).to(device)\n",
    "\n",
    "    if true_relation in ['right of', 'in-front of']:\n",
    "        f1, f2 = f2, f1\n",
    "    \n",
    "    prompt1 = f\"USER: <image>\\nIs a {f1} in the image?\\nASSISTANT:\"\n",
    "    prompt2 = f\"USER: <image>\\nIs a {f2} in the image?\\nASSISTANT:\"\n",
    "\n",
    "    correct1, correct2 = llava_vqa(image1, prompt1, device), llava_vqa(image2, prompt2, device)\n",
    "    return correct1, correct2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256 256\n",
      "376 376\n",
      "632 632\n",
      "888 888\n"
     ]
    }
   ],
   "source": [
    "handle = \"0228_195834\"\n",
    "whichset=\"test\"\n",
    "sample_dir = f\"../scripts/diffuser_real/output/{handle}/infr/{whichset}_sentences\"\n",
    "epc = sorted([f for f in os.listdir(sample_dir)], key=lambda x: int(x[5:].split(\"_\")[0]))[-1]\n",
    "imsize = 64\n",
    "\n",
    "pilimages, gth_captions = [], []\n",
    "\n",
    "for f in os.listdir(f\"{sample_dir}/{epc}/samples\"):\n",
    "    if \".txt\" in f: continue\n",
    "    im = Image.open(f\"{sample_dir}/{epc}/samples/{f}\")\n",
    "    W, H = im.size\n",
    "    nrows, ncols = H//imsize, W//imsize\n",
    "    captions_file = f.replace(\".png\", \".txt\")\n",
    "    with open(f\"{sample_dir}/{epc}/samples/{captions_file}\", \"r\") as captions:\n",
    "        gth_captions.extend([x.strip() for x in captions.readlines()])\n",
    "    for r in range(nrows):\n",
    "        for c in range(ncols):\n",
    "            left, top = c*imsize, r*imsize\n",
    "            right, bottom = left+imsize, top+imsize\n",
    "            pilimage = im.crop((left, top, right, bottom))\n",
    "            if np.sum(pilimage) == 255*3*64*64: continue # skip placeholders which are purely white images\n",
    "            pilimages.append(pilimage)\n",
    "    print(len(pilimages), len(gth_captions))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 888/888 [1:01:20<00:00,  4.14s/it]\n"
     ]
    }
   ],
   "source": [
    "correct = []\n",
    "for pilimage, gth_caption in tqdm(zip(pilimages, gth_captions), total=len(gth_captions)):\n",
    "    correct1, correct2 = evaluate(gth_caption, pilimage, device)\n",
    "    correct.append(correct1 and correct2)\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0653"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(sum(correct) / len(correct), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FN = {'can': 6, 'candle': 1, 'tape': 32, 'bowl': 5, 'fork': 11, 'flower': 27, 'sunglasses': 8, 'spoon': 5, 'remote': 3, 'plate': 3, 'scissors': 11, 'knife': 11, 'phone': 1}\n",
    "FP = {('mug', 'cup'): 63, ('knife', 'scissors'): 5, ('bowl', 'plate'): 27, ('can', 'remote'): 6, \n",
    "      ('knife', 'fork'): 8, ('can', 'candle'): 11, ('cup', 'can'): 5, ('cap', 'book'): 6, \n",
    "      ('candle', 'cap'): 18, ('candle', 'bowl'): 2, ('tape', 'remote'): 4, ('tape', 'headphones'): 5, \n",
    "      ('spoon', 'cap'): 4, ('sunglasses', 'headphones'): 27, ('knife', 'remote'): 2, \n",
    "      ('knife', 'phone'): 4, ('knife', 'spoon'): 9, ('can', 'mug'): 4, ('can', 'headphones'): 3, \n",
    "      ('can', 'phone'): 1, ('can', 'book'): 1, ('cap', 'phone'): 2, ('scissors', 'cap'): 19, \n",
    "      ('scissors', 'remote'): 22, ('scissors', 'headphones'): 21, ('scissors', 'knife'): 17, \n",
    "      ('bowl', 'mug'): 4, ('headphones', 'mug'): 1, ('bowl', 'cup'): 10, ('headphones', 'cup'): 1, \n",
    "      ('can', 'cap'): 33, ('cup', 'plate'): 1, ('can', 'tape'): 1, ('book', 'mug'): 3, \n",
    "      ('book', 'cap'): 2, ('cap', 'tape'): 7, ('book', 'cup'): 4, ('book', 'can'): 1, \n",
    "      ('plate', 'spoon'): 8, ('flower', 'candle'): 21, ('sunglasses', 'candle'): 4, \n",
    "      ('sunglasses', 'phone'): 4, ('flower', 'cap'): 13, ('bowl', 'remote'): 4, \n",
    "      ('flower', 'scissors'): 2, ('spoon', 'knife'): 2, ('spoon', 'scissors'): 5, \n",
    "      ('headphones', 'remote'): 10, ('remote', 'tape'): 4, ('bowl', 'candle'): 3, \n",
    "      ('remote', 'phone'): 16, ('sunglasses', 'mug'): 3, ('sunglasses', 'cup'): 3, \n",
    "      ('bowl', 'phone'): 1, ('plate', 'mug'): 6, ('can', 'plate'): 1, ('plate', 'cup'): 7, \n",
    "      ('tape', 'mug'): 1, ('tape', 'spoon'): 4, ('tape', 'cup'): 3, ('scissors', 'phone'): 7, \n",
    "      ('scissors', 'spoon'): 4, ('mug', 'candle'): 1, ('remote', 'candle'): 3, ('scissors', 'tape'): 7, \n",
    "      ('remote', 'headphones'): 24, ('plate', 'bowl'): 13, ('sunglasses', 'remote'): 9, \n",
    "      ('sunglasses', 'can'): 1, ('sunglasses', 'cap'): 9, ('fork', 'remote'): 7, \n",
    "      ('fork', 'headphones'): 5, ('fork', 'phone'): 3, ('spoon', 'remote'): 5, ('candle', 'remote'): 3, \n",
    "      ('spoon', 'fork'): 2, ('spoon', 'candle'): 7, ('spoon', 'bowl'): 1, ('spoon', 'cup'): 1, \n",
    "      ('plate', 'cap'): 3, ('candle', 'cup'): 1, ('can', 'knife'): 2, ('cap', 'mug'): 3, \n",
    "      ('cap', 'cup'): 1, ('headphones', 'cap'): 9, ('headphones', 'phone'): 4, \n",
    "      ('sunglasses', 'scissors'): 1, ('fork', 'spoon'): 6, ('fork', 'knife'): 4, \n",
    "      ('fork', 'scissors'): 5, ('remote', 'can'): 1, ('cap', 'remote'): 1, ('can', 'cup'): 4, \n",
    "      ('cap', 'flower'): 7, ('flower', 'remote'): 1, ('flower', 'knife'): 2, ('bowl', 'spoon'): 3, \n",
    "      ('mug', 'phone'): 1, ('bowl', 'cap'): 1, ('scissors', 'fork'): 3, ('phone', 'remote'): 2, \n",
    "      ('phone', 'headphones'): 2, ('plate', 'can'): 1, ('tape', 'cap'): 2, ('spoon', 'plate'): 1, \n",
    "      ('candle', 'mug'): 2, ('candle', 'spoon'): 1, ('candle', 'can'): 1, ('sunglasses', 'book'): 2, \n",
    "      ('fork', 'cap'): 3, ('plate', 'remote'): 1, ('plate', 'phone'): 1, ('remote', 'cap'): 7, \n",
    "      ('can', 'scissors'): 1, ('remote', 'mug'): 2, ('scissors', 'candle'): 1, ('scissors', 'can'): 1, \n",
    "      ('candle', 'phone'): 1, ('book', 'remote'): 1, ('book', 'tape'): 1, ('cup', 'cap'): 1, \n",
    "      ('book', 'candle'): 1, ('remote', 'book'): 1, ('can', 'spoon'): 1, ('flower', 'plate'): 2, \n",
    "      ('flower', 'bowl'): 1, ('flower', 'cup'): 4, ('candle', 'plate'): 1, ('remote', 'cup'): 1, \n",
    "      ('tape', 'phone'): 1, ('knife', 'tape'): 1, ('bowl', 'flower'): 1, ('flower', 'mug'): 2, \n",
    "      ('headphones', 'tape'): 1, ('headphones', 'knife'): 1, ('headphones', 'scissors'): 1, \n",
    "      ('headphones', 'candle'): 3, ('remote', 'plate'): 1, ('remote', 'bowl'): 1, \n",
    "      ('headphones', 'can'): 1, ('spoon', 'flower'): 3, ('spoon', 'headphones'): 3, \n",
    "      ('spoon', 'tape'): 1, ('spoon', 'phone'): 1, ('tape', 'bowl'): 1, ('flower', 'can'): 1, \n",
    "      ('cap', 'candle'): 1, ('sunglasses', 'plate'): 1, ('sunglasses', 'bowl'): 1, \n",
    "      ('book', 'headphones'): 1, ('book', 'phone'): 1, ('headphones', 'sunglasses'): 1, \n",
    "      ('remote', 'flower'): 1, ('knife', 'plate'): 1, ('knife', 'bowl'): 1, ('remote', 'scissors'): 1, \n",
    "      ('fork', 'plate'): 1, ('fork', 'bowl'): 1, ('fork', 'cup'): 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "748"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(list(FP.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava",
   "language": "python",
   "name": "llava"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
