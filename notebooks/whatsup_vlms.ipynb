{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os, sys, random, re, math\n",
    "sys.path.append(\"../scripts/formalism\")\n",
    "from entropy import *\n",
    "from formalism_utils import *\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, trange\n",
    "from pprint import pprint\n",
    "from textblob import Word\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from plottable import Table, ColumnDefinition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rewrite Dataset to Convenient Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "408\n"
     ]
    }
   ],
   "source": [
    "version = \"b\"\n",
    "annotations = []\n",
    "if 'a' in version:\n",
    "    annotations.extend(json.load(open(\"../whatsup_vlms/data/controlled_images_dataset.json\", \"r\")))\n",
    "if 'b' in version:\n",
    "    annotations.extend(json.load(open(\"../whatsup_vlms/data/controlled_clevr_dataset.json\", \"r\")))\n",
    "\n",
    "print(len(annotations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image_path': 'data/controlled_clevr/mug_right_of_knife.jpeg',\n",
       " 'caption_options': ['A mug to the right of a knife',\n",
       "  'A mug in front of a knife',\n",
       "  'A mug behind a knife',\n",
       "  'A mug to the left of a knife']}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#unique objects from full What'sUp:  18\n"
     ]
    }
   ],
   "source": [
    "s = set()\n",
    "for a in annotations:\n",
    "    tmp = a[\"image_path\"].split(\"/\")[-1].split(\".\")[0].split(\"_\")\n",
    "    o1, o2 = tmp[0], tmp[-1]\n",
    "    s.add(o1)\n",
    "    s.add(o2)\n",
    "print(\"#unique objects from full What'sUp: \", len(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### write to json with the same format as \"vgr_nocaps_tb_both_complete.json\"\n",
    "J = []\n",
    "SUBJ, OBJ = [], []\n",
    "for a in tqdm(annotations):\n",
    "    filename = a['image_path'][5:]\n",
    "    tmp = a['image_path'].split(\"/\")[-1][:-5].split(\"_\")\n",
    "    subj, obj = tmp[0], tmp[-1]\n",
    "    SUBJ.append(subj)\n",
    "    OBJ.append(obj)\n",
    "    r = \" \".join(tmp[1:-1])\n",
    "    J.append([\n",
    "        a['caption_options'][0],\n",
    "        \"whatsup_vlms/\" + filename,\n",
    "        (subj, obj, r)\n",
    "    ])\n",
    "print(len(J))\n",
    "print(f\"#unique subj = \", len(set(SUBJ)))\n",
    "print(f\"#unique obj = \", len(set(OBJ)))\n",
    "print(f\"#unique concepts = \", len(set(SUBJ).union(set(OBJ))))\n",
    "print(f\"#unique train_triplets = \", len(set([a[-1] for a in J])))\n",
    "json.dump(J, open(f\"../data/aggregated/whatsup_vlm_{version}.json\", \"w\"), indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYMMETRIC_REL = {\n",
    "    \"left of\": \"right of\",\n",
    "    \"right of\": \"left of\", \n",
    "    \"in-front of\": \"behind\", \n",
    "    \"behind\": \"in-front of\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 408/408 [00:00<00:00, 708896.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "autofill 154 tuples\n",
      "\n",
      "308\n",
      "#unique subj =  7\n",
      "#unique obj =  14\n",
      "#unique concepts =  15\n",
      "#unique train_triplets =  308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### write to json with the same format as \"vgr_nocaps_tb_both_complete.json\"\n",
    "### toggle autofill --- only works for version b\n",
    "J = []\n",
    "SUBJ, OBJ = [], []\n",
    "assert version == \"b\"\n",
    "autofill_symmetric_rel = True\n",
    "skip_nouns = [\"sunglasses\", \"remote\", \"phone\"] # None #\n",
    "rel_version = \"lr\"\n",
    "\n",
    "suffix = \"_autofill\" if autofill_symmetric_rel else \"\"\n",
    "if skip_nouns is not None: suffix += \"_remove_\" + \"_\".join([x[:3] for x in skip_nouns])\n",
    "for a in tqdm(annotations):\n",
    "    filename = a['image_path'][5:]\n",
    "    tmp = a['image_path'].split(\"/\")[-1][:-5].split(\"_\")\n",
    "    subj, obj = tmp[0], tmp[-1]\n",
    "    if skip_nouns is not None and (subj in skip_nouns or obj in skip_nouns): continue\n",
    "    SUBJ.append(subj)\n",
    "    OBJ.append(obj)\n",
    "    r = \" \".join(tmp[1:-1])\n",
    "    if rel_version == \"lr\" and r in [\"in-front of\", \"behind\"]: continue\n",
    "    if rel_version == \"fb\" and r in ['left of', \"right of\"]: continue\n",
    "    J.append([\n",
    "        a['caption_options'][0],\n",
    "        \"whatsup_vlms/\" + filename,\n",
    "        (subj, obj, r)\n",
    "    ])\n",
    "\n",
    "if autofill_symmetric_rel:\n",
    "    autofill = []\n",
    "    for a in J:\n",
    "        subj, obj, r = a[-1]\n",
    "        change_r = a[0].replace(r, SYMMETRIC_REL[r])\n",
    "        tmp = change_r.split()\n",
    "        tmp[1] = obj\n",
    "        tmp[-1] = subj\n",
    "        autofill.append([\n",
    "            \" \".join(tmp),\n",
    "            a[1],\n",
    "            (obj, subj, SYMMETRIC_REL[r])\n",
    "        ])\n",
    "        \n",
    "    print(f\"\\nautofill {len(autofill)} tuples\\n\")\n",
    "    J.extend(autofill)\n",
    "\n",
    "print(len(J))\n",
    "print(f\"#unique subj = \", len(set(SUBJ)))\n",
    "print(f\"#unique obj = \", len(set(OBJ)))\n",
    "print(f\"#unique concepts = \", len(set(SUBJ).union(set(OBJ))))\n",
    "print(f\"#unique train_triplets = \", len(set([a[-1] for a in J])))\n",
    "json.dump(J, open(f\"../data/aggregated/whatsup_vlm_{version}_{rel_version}{suffix}.json\", \"w\"), indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_funcs = [\n",
    "    #\"concept_centric_entropy3(num_nouns, num_relations, df)\",\n",
    "    #\"concept_centric_entropy4(num_nouns, num_relations, df)\",\n",
    "    #\"concept_centric_entropy5(num_nouns, num_relations, df)\",\n",
    "    #\"relation_centric_entropy3(num_nouns, num_relations, df)\",\n",
    "    #\"relation_centric_entropy4(num_nouns, num_relations, df)\",\n",
    "    #\"relation_centric_entropy5(num_nouns, num_relations, df)\",\n",
    "    #\"divergence(num_nouns, num_relations, df)\",\n",
    "    #\"divergence2(num_nouns, num_relations, df)\",\n",
    "    #\"divergence3(num_nouns, num_relations, df)\",\n",
    "    #\"concept_role_entropy(num_nouns, df)\",\n",
    "    #\"concept_role_entropy2(num_nouns, df)\",\n",
    "    #\"role_association(num_nouns, df)\",\n",
    "    #\"role_association2(num_nouns, df)\",\n",
    "    #\"concept_entropy(num_nouns, df)\",\n",
    "    #\"concept_entropy2(num_nouns, df)\",\n",
    "    #\"concept_entropy0(num_nouns, df)\",\n",
    "    \"concept_role_index_entropy(num_nouns, df)\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#unique concepts =  15\n",
      "relations = {'right of', 'left of'}\n",
      "\n",
      "2 15\n",
      "#unique O1 =  15\n",
      "#unique O2 =  15\n",
      "#unique train_triplets =  308\n",
      "role intrinsic meanings: linguistic position \n",
      "\n",
      "concept_role_index_entropy :  0.6931471805599453\n",
      "\n",
      "#unique O1 =  15\n",
      "#unique O2 =  15\n",
      "#unique train_triplets =  308\n",
      "role intrinsic meanings: image position \n",
      "\n",
      "concept_role_index_entropy :  0.6931471805599453\n",
      "\n"
     ]
    }
   ],
   "source": [
    "version = \"b_lr_autofill_remove_sun_rem_pho\"\n",
    "rel = \"lr\"\n",
    "autofill_symmetric_rel = False\n",
    "map = {\n",
    "    \"lr\": [\"left of\", \"right of\"],\n",
    "    \"fb\": [\"in-front of\", \"behind\"]\n",
    "}\n",
    "\n",
    "annotations = json.load(open(f\"../data/aggregated/whatsup_vlm_{version}.json\", \"r\"))\n",
    "SUBJ, OBJ, tuples, relations = [], [], [], map[rel]\n",
    "for a in annotations:\n",
    "    subj, obj, r = a[-1]\n",
    "    if not r in relations: continue\n",
    "    SUBJ.append(subj)\n",
    "    OBJ.append(obj)\n",
    "    tuples.append((subj, obj, r))\n",
    "#print(f\"#unique subj = \", len(set(SUBJ)))\n",
    "#print(f\"#unique obj = \", len(set(OBJ)))\n",
    "nouns = sorted(list(set(SUBJ).union(set(OBJ))))\n",
    "num_nouns, num_relations = len(nouns), len(relations)\n",
    "print(\"#unique concepts = \", num_nouns)\n",
    "print(f\"relations = {set(relations)}\\n\")\n",
    "\n",
    "n2i = {n:i for i, n in enumerate(nouns)}\n",
    "r2i = {r:i for i, r in enumerate(relations)}\n",
    "print(len(r2i), len(n2i))\n",
    "\n",
    "if autofill_symmetric_rel:\n",
    "    autofill = []\n",
    "    for t in tuples:\n",
    "        subj, obj, r = t\n",
    "        if not (obj, subj, relations[1-r2i[r]]) in tuples:\n",
    "            autofill.append((obj, subj, relations[1-r2i[r]]))\n",
    "    print(f\"\\nautofill {len(autofill)} tuples\\n\")\n",
    "    tuples.extend(autofill)\n",
    "\n",
    "for transpose in [False, True]:\n",
    "    train_triplets = [] # convert tuple elements to indices\n",
    "    for subj, obj, r in tuples:\n",
    "        train_triplets.append((n2i[subj], n2i[obj], r2i[r]))\n",
    "    if transpose: train_triplets = Transpose(train_triplets)\n",
    "\n",
    "    print(\"#unique O1 = \", len(set([t[0] for t in train_triplets])))\n",
    "    print(\"#unique O2 = \", len(set([t[1] for t in train_triplets])))\n",
    "    print(f\"#unique train_triplets = \", len(set(train_triplets)))\n",
    "\n",
    "    df = pd.DataFrame(train_triplets, columns =['O1', 'O2', 'R'])\n",
    "    print(\"role intrinsic meanings: {} position {}\\n\".format(\"image\" if transpose else \"linguistic\", \"(with autofill)\" if autofill_symmetric_rel else \"\"))\n",
    "    for f in entropy_funcs:\n",
    "        score = eval(f)\n",
    "        print(f.split(\"(\")[0], \": \", score)\n",
    "    print()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### visualize table\n",
    "# Put the most full column on the left\n",
    "column_names = sorted(nouns, key=lambda n: len([t for t in train_triplets if nouns[t[0]]==n]), reverse=True)\n",
    "df = pd.DataFrame(columns=column_names, index=column_names)\n",
    "for t in train_triplets:\n",
    "    if not isinstance(df[nouns[t[0]]][nouns[t[1]]], str):\n",
    "        df[nouns[t[0]]][nouns[t[1]]] = \"R1\\n  \" if t[-1]==0 else \"  \\nR2\"\n",
    "    else:\n",
    "        if t[-1] == 0: df[nouns[t[0]]][nouns[t[1]]] = \"R1\" + df[nouns[t[0]]][nouns[t[1]]][2:]\n",
    "        else: df[nouns[t[0]]][nouns[t[1]]] = df[nouns[t[0]]][nouns[t[1]]][:3] + \"R2\"\n",
    "for n in column_names:\n",
    "    for m in column_names:\n",
    "        if not isinstance(df[n][m], str): df[n][m] = \"\"\n",
    "col_defs = [ColumnDefinition(\n",
    "            name=n,\n",
    "            title=n.replace(\"glasses\", \"\\nglasses\").replace(\"phones\", \"\\nphones\"),\n",
    "            border=\"left\",\n",
    "            textprops={\"ha\": \"center\"},\n",
    "        ) for n in column_names]\n",
    "fig, ax = plt.subplots(figsize=(15, 12))\n",
    "tab = Table(df,\n",
    "            ax=ax,\n",
    "            column_definitions = col_defs,\n",
    "            column_border_kw={\"linewidth\": 1, \"color\": \"black\", \"linestyle\": \"-\"},\n",
    "            footer_divider=True,\n",
    "            )\n",
    "plt.title(\"{} positions {} {}\".format(\"image\" if transpose else \"linguistic\", \n",
    "                                   \"(with autofill)\" if autofill_symmetric_rel else \"\",\n",
    "                                   relations), \n",
    "                                   fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save example images\n",
    "from diffusers.utils import make_image_grid\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "timezone = pytz.timezone('America/New_York') \n",
    "date = datetime.now(timezone).strftime(\"%m%d_%H%M%S\")\n",
    "example_dir = \"../whatsup_vlms/data/img_examples/\"\n",
    "imsize = 64\n",
    "images, texts = [], []\n",
    "for e in random.sample(annotations, 32):\n",
    "    images.append(Image.open(os.path.join(\n",
    "        \"/data/yingshac/clevr_control/data/\",\n",
    "        e[1]\n",
    "    )).convert(\"RGB\").resize((imsize, imsize)))\n",
    "    texts.append(e[0])\n",
    "\n",
    "image_grid = make_image_grid(images, rows=16, cols=math.ceil(len(images)/16))\n",
    "image_grid.save(\"{}/{}.png\".format(example_dir, date))\n",
    "with open(\"{}/{}.txt\".format(example_dir, date), \"w\") as f:\n",
    "    f.write(\"\\n\".join(texts))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../scripts/\")\n",
    "from diffuser_real.dataset import real_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = json.load(open(\"../data/aggregated/whatsup_vlm_b_lr_autofill_remove_sun_rem_pho.json\", \"r\"))\n",
    "imdir=\"/data/yingshac/clevr_control/data/\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154\n",
      "Coverage = 0.37\n"
     ]
    }
   ],
   "source": [
    "split = \"splitI\"\n",
    "transpose = ord(split[-1]) - ord('A') > 13\n",
    "D = real_dataset(imdir, annotations, imsize=(32, 64), subsample_method=f\"subsample_whatsup_{split}\")\n",
    "print(len(D))\n",
    "n2i = {n:i for i, n in enumerate(D.classes)}\n",
    "r2i = {\"left of\": 0, \"right of\": 1} # hardcoded\n",
    "print(f\"Coverage = {round(len(D)/(len(n2i)*(len(n2i)-1)*len(r2i)), 2)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#unique images = 154/154\n",
      "Linguistic Balance\n",
      "concept_role_index_entropy : 0.5524, normalized: 0.8\n",
      "Image Balance\n",
      "concept_role_index_entropy : 0.6931, normalized: 1.0\n"
     ]
    }
   ],
   "source": [
    "train_triplets = [(n2i[d[-1][0]], n2i[d[-1][1]], r2i[d[-1][-1]]) for d in D.data]\n",
    "\n",
    "tmp = train_triplets if transpose else Transpose(train_triplets, apply_to_relations=[1])\n",
    "print(\"#unique images = {}/154\".format(len(set([x[:2] for x in tmp]))))\n",
    "\n",
    "df = pd.DataFrame(train_triplets, columns =['O1', 'O2', 'R'])\n",
    "num_nouns = len(D.classes)\n",
    "score = concept_role_index_entropy(num_nouns, df)\n",
    "print(\"Linguistic Balance\")\n",
    "print(f\"concept_role_index_entropy : {round(score, 4)}, normalized: {round(score/np.log(2), 2)}\")\n",
    "#if transpose: \n",
    "\n",
    "train_triplets = Transpose(train_triplets, apply_to_relations=[1])\n",
    "df = pd.DataFrame(train_triplets, columns =['O1', 'O2', 'R'])\n",
    "num_nouns = len(D.classes)\n",
    "score = concept_role_index_entropy(num_nouns, df)\n",
    "print(\"Image Balance\")\n",
    "#print(\"{} Balance\".format(\"Image\" if transpose else \"Linguistic\"))\n",
    "print(f\"concept_role_index_entropy : {round(score, 4)}, normalized: {round(score/np.log(2), 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 77, 0: 77})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter([x[-1] for x in train_triplets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuples = [d[-1] for d in D.data]\n",
    "[\"tape\", \"book\", \"right of\"] in tuples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from typing import Any, Dict, Optional, Tuple, Union, List\n",
    "from torchvision import transforms\n",
    "\n",
    "class whatsup_singleobj_dataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 imdir: str,\n",
    "                 annotations: List,\n",
    "                 imsize = (32, 32),\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.imdir = imdir\n",
    "        self.annotations = annotations\n",
    "        self.preprocess = transforms.Compose(\n",
    "            [   \n",
    "                transforms.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
    "                transforms.Resize(imsize),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        print(\"Data Preprocessing\")\n",
    "        self.concept2pilimages = defaultdict(list)\n",
    "        for a in tqdm(self.annotations):\n",
    "            gth_caption, image_path, T = a\n",
    "            image = Image.open(os.path.join(self.imdir, image_path))\n",
    "\n",
    "            width, height = image.size\n",
    "            new_dimension = min(image.size)\n",
    "            left = (width - new_dimension)/2\n",
    "            top = (height - new_dimension)/2\n",
    "            right = (width + new_dimension)/2\n",
    "            bottom = (height + new_dimension)/2\n",
    "            pilimage = image.crop((left, top, right, bottom))\n",
    "\n",
    "            W, H = pilimage.size\n",
    "            crop1 = pilimage.crop((W//4, 0, 3*W//4, H//2)) # behind\n",
    "            crop2 = pilimage.crop((W//4, H//2, 3*W//4, H)) # front\n",
    "            crop3 = pilimage.crop((0, H//4, W//2, 3*H//4)) # left\n",
    "            crop4 = pilimage.crop((W//2, H//4, W, 3*H//4)) # right\n",
    "            f1, f2, r = T\n",
    "\n",
    "            if \"left of\" in r: \n",
    "                self.concept2pilimages[f1].append(crop3)\n",
    "                self.concept2pilimages[f2].append(crop4)\n",
    "            elif \"right of\" in r: \n",
    "                self.concept2pilimages[f2].append(crop3)\n",
    "                self.concept2pilimages[f1].append(crop4)\n",
    "            elif \"in-front of\" in r: \n",
    "                self.concept2pilimages[f2].append(crop1)\n",
    "                self.concept2pilimages[f1].append(crop2)\n",
    "            elif \"behind\" in r: \n",
    "                self.concept2pilimages[f1].append(crop1)\n",
    "                self.concept2pilimages[f2].append(crop2)\n",
    "            else: raise ValueError(f\"Invalid relation: {r}\")\n",
    "\n",
    "        self.classes = sorted(self.concept2pilimages.keys(), key=lambda x: len(self.concept2pilimages[x]), reverse=True)\n",
    "\n",
    "        print(\"Finish Preprocessing\")\n",
    "        for k in self.classes:\n",
    "            print(f\"concept {k} has {len(self.concept2pilimages[k])} crops\")\n",
    "        \n",
    "    def __len__(self): return len(self.classes)\n",
    "    \n",
    "    def __getitem__(self, i): \n",
    "        f = self.classes[i]\n",
    "        image = random.choice(self.concept2pilimages[f])\n",
    "        print(\"in getitem, image.size = \", image.size)\n",
    "        text = f\"an image of a {f}\"\n",
    "        return {\n",
    "            'image': image, #self.preprocess(image),\n",
    "            'sentence': text\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = json.load(open(\"../data/aggregated/whatsup_vlm_b_lr.json\", \"r\"))\n",
    "annotations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = whatsup_singleobj_dataset(\n",
    "    imdir=\"/data/yingshac/clevr_control/data/\",\n",
    "    annotations=annotations,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = random.choice(list(range(len(D))))\n",
    "i=17\n",
    "print(D[i]['sentence'])\n",
    "D[i]['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, torch, json, random\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from typing import Any, Dict, Optional, Tuple, Union, List\n",
    "from tqdm import tqdm, trange\n",
    "from collections import defaultdict, Counter\n",
    "class dataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 imdir: str,\n",
    "                 data: List,\n",
    "                 imsize = (64, 64),\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.imdir = imdir\n",
    "        self.data = data\n",
    "        self.preprocess = transforms.Compose(\n",
    "            [   \n",
    "                transforms.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
    "                transforms.Resize(imsize),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "    def __len__(self): return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, i): \n",
    "        text, image_path, tuples = self.data[i]\n",
    "        image = Image.open(os.path.join(self.imdir, image_path))\n",
    "        width, height = image.size\n",
    "        new_dimension = min(image.size)\n",
    "        left = (width - new_dimension)/2\n",
    "        top = (height - new_dimension)/2\n",
    "        right = (width + new_dimension)/2\n",
    "        bottom = (height + new_dimension)/2\n",
    "        image = image.crop((left, top, right, bottom))\n",
    "        #print(image.size)\n",
    "        W, H = image.size\n",
    "        assert W==H\n",
    "\n",
    "        r = tuples[-1]\n",
    "        if r in [\"left of\", \"right of\"]: image = image.crop((0, H//4, W, 3*H//4))\n",
    "        elif r in [\"in-front of\", \"behind\"]: image = image.crop((W//4, 0, 3*W//4, H))\n",
    "        \n",
    "        return {\n",
    "            'image': image,\n",
    "            'sentence': text\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = json.load(open(\"../data/aggregated/whatsup_vlm_b_lr_autofill.json\", \"r\"))\n",
    "imdir=\"/data/yingshac/clevr_control/data/\" \n",
    "D = dataset(imdir, annotations, imsize=(32, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = random.choice(list(range(len(D))))\n",
    "print(D[i]['sentence'])\n",
    "D[i]['image']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete Useless Ckpts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "dir = \"/data/yingshac/clevr_control/scripts/diffuser_real/output/0228_115732/ckpts\"\n",
    "for f in os.listdir(dir):\n",
    "    if int(f.split(\"_\")[0]) % 10 != 9:\n",
    "        x = os.path.join(dir, f)\n",
    "        print(x)\n",
    "        #os.remove(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "dir = \"/data/yingshac/clevr_control/scripts/diffuser_real/output\"\n",
    "for handle in sorted(os.listdir(dir)):\n",
    "    #if int(\"\".join(handle.split(\"_\"))) < 228115732:\n",
    "    print(handle)\n",
    "#for f in os.listdir(dir):\n",
    "#    if int(f.split(\"_\")[0]) % 10 != 9:\n",
    "#        x = os.path.join(dir, f)\n",
    "#        print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
