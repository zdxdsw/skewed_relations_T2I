{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image\n",
    "from typing import Any, Dict, Optional, Tuple, Union, List\n",
    "import os, json, random\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To ensure we get reproducible results\n",
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "randaug = transforms.RandomApply(transforms=[\n",
    "    #transforms.RandomRotation(20, interpolation=Image.BILINEAR),\n",
    "    transforms.ColorJitter(brightness=.3, contrast=.3, saturation=0.3, hue=0),\n",
    "    #transforms.RandomPerspective(distortion_scale=0.8, p=0.6),\n",
    "], p=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 classes: List[str], # used to define class_ids, order matters\n",
    "                 imdir: str,\n",
    "                 data: List,\n",
    "                 imsize = (64, 64),\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.imdir = imdir\n",
    "        self.data = data\n",
    "        self.preprocess = transforms.Compose(\n",
    "            [   \n",
    "                transforms.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
    "                randaug,\n",
    "                transforms.RandomResizedCrop(imsize, scale=(0.8, 1.0), ratio=(0.5, 1.5)),\n",
    "                #transforms.Resize(imsize),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ]\n",
    "        )\n",
    "        self.classes = classes\n",
    "        #self.n2i = {n:i+1 for i, n in enumerate(classes)}\n",
    "\n",
    "    def __len__(self): return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, i): \n",
    "        text, image_path, tuple = self.data[i]\n",
    "        image = Image.open(os.path.join(self.imdir, image_path))\n",
    "        subj, obj, r = tuple\n",
    "        label = torch.zeros((18, ), dtype=torch.int64)\n",
    "        for i in range(len(self.classes)):\n",
    "            if self.classes[i] in [subj, obj]:\n",
    "                label[i] = 1\n",
    "\n",
    "        return {\n",
    "            'sentence': text,\n",
    "            'image': self.preprocess(image),\n",
    "            'label': label\n",
    "        }\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc(output, gth, num_classes):\n",
    "    # output: (bs*num_classes, 2)\n",
    "    # gth: (bs*num_classes,)\n",
    "    pred = output.argmax(1, keepdim=True).reshape((-1, num_classes))\n",
    "    gth = gth.reshape((-1, num_classes))\n",
    "    correct = sum(pred.eq(gth).sum(axis=1) == num_classes)\n",
    "    acc = correct.float() / gth.shape[0]\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, dataloader, optimizer, criterion, num_classes, device):\n",
    "    net.train()\n",
    "    running_loss, running_acc = [], []\n",
    "    for batch in dataloader:\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        bs = len(batch['image'])\n",
    "        outputs = net(batch['image'].to(device)).reshape((bs*num_classes, -1)) \n",
    "\n",
    "        labels = batch['label'].flatten().to(device) # (bs*num_classes, )\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        acc = get_acc(outputs, labels, num_classes)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss.append(loss.item())\n",
    "        running_acc.append(acc.item())\n",
    "    return np.mean(running_loss), np.mean(running_acc)\n",
    "\n",
    "def val(net, dataloader, criterion, num_classes, device):\n",
    "    net.eval()\n",
    "    epoch_loss, epoch_acc = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            bs = len(batch['image'])\n",
    "            outputs = net(batch['image'].to(device)).reshape((bs*num_classes, -1)) \n",
    "\n",
    "            labels = batch['label'].flatten().to(device)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            acc = get_acc(outputs, labels, num_classes)\n",
    "\n",
    "            # print statistics\n",
    "            epoch_loss.append(loss.item())\n",
    "            epoch_acc.append(acc.item())\n",
    "        \n",
    "    return np.mean(epoch_loss), np.mean(epoch_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326 training examples, 82 testing examples\n"
     ]
    }
   ],
   "source": [
    "annotations = json.load(open(\"../data/aggregated/whatsup_vlm_b.json\", \"r\"))\n",
    "occurrences = [a[-1][0] for a in annotations] + [a[-1][1] for a in annotations]\n",
    "\n",
    "c = Counter(occurrences)\n",
    "classes = sorted(c.keys(), key=lambda x: (-c[x], x))\n",
    "D = dataset(\n",
    "    classes, \n",
    "    imdir = \"/data/yingshac/clevr_control/data/\",\n",
    "    data = annotations\n",
    ")\n",
    "\n",
    "device=\"cuda:0\"\n",
    "\n",
    "train_ratio = 0.8\n",
    "train_num = int(train_ratio*len(D))\n",
    "val_num = len(D) - train_num\n",
    "train_data, val_data = random_split(D, [train_num, val_num])\n",
    "print(f\"{len(train_data)} training examples, {len(val_data)} testing examples\")\n",
    "\n",
    "num_classes = len(D.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_classes: int,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.classifiers = nn.ModuleList([nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), \n",
    "            nn.Conv2d(64, 128, 3), \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), \n",
    "            nn.Conv2d(128, 128, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(128, 8, 6), # (bs, 8, 1, 1)\n",
    "            nn.Flatten(1), # (bs, 8)\n",
    "            nn.Linear(8, 2, bias=False) # (bs, 2)\n",
    "        ) for i in range(num_classes)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = []\n",
    "        for c in self.classifiers:\n",
    "            #y = x\n",
    "            #for layer in c:\n",
    "                #y = layer(y)\n",
    "            res.append(c(x)) # (bs, 2)\n",
    "        res = torch.stack(res, axis=0).transpose(0, 1) # (bs, num_classes, 2)\n",
    "        return res\n",
    "\n",
    "\n",
    "net = Net(len(D.classes)).to(device) # +1 for black_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "trainloader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "testloader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1).to(device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001) #, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01\n",
      "\tTrain Loss: 0.483 | Train Acc: 0.00%\n",
      "\t Val. Loss: 0.421 |  Val. Acc: 0.00%\n",
      "Epoch: 02\n",
      "\tTrain Loss: 0.407 | Train Acc: 0.30%\n",
      "\t Val. Loss: 0.398 |  Val. Acc: 0.00%\n",
      "Epoch: 03\n",
      "\tTrain Loss: 0.383 | Train Acc: 1.19%\n",
      "\t Val. Loss: 0.380 |  Val. Acc: 1.04%\n",
      "Epoch: 04\n",
      "\tTrain Loss: 0.363 | Train Acc: 1.69%\n",
      "\t Val. Loss: 0.371 |  Val. Acc: 2.08%\n",
      "Epoch: 05\n",
      "\tTrain Loss: 0.351 | Train Acc: 6.55%\n",
      "\t Val. Loss: 0.348 |  Val. Acc: 7.29%\n",
      "Epoch: 06\n",
      "\tTrain Loss: 0.327 | Train Acc: 11.31%\n",
      "\t Val. Loss: 0.341 |  Val. Acc: 9.38%\n",
      "Epoch: 07\n",
      "\tTrain Loss: 0.318 | Train Acc: 12.50%\n",
      "\t Val. Loss: 0.327 |  Val. Acc: 8.33%\n",
      "Epoch: 08\n",
      "\tTrain Loss: 0.307 | Train Acc: 18.85%\n",
      "\t Val. Loss: 0.316 |  Val. Acc: 21.88%\n",
      "Epoch: 09\n",
      "\tTrain Loss: 0.299 | Train Acc: 26.39%\n",
      "\t Val. Loss: 0.315 |  Val. Acc: 15.62%\n",
      "Epoch: 10\n",
      "\tTrain Loss: 0.291 | Train Acc: 28.17%\n",
      "\t Val. Loss: 0.312 |  Val. Acc: 25.00%\n",
      "Epoch: 11\n",
      "\tTrain Loss: 0.285 | Train Acc: 32.44%\n",
      "\t Val. Loss: 0.316 |  Val. Acc: 17.71%\n",
      "Epoch: 12\n",
      "\tTrain Loss: 0.276 | Train Acc: 40.18%\n",
      "\t Val. Loss: 0.301 |  Val. Acc: 17.71%\n",
      "Epoch: 13\n",
      "\tTrain Loss: 0.276 | Train Acc: 40.08%\n",
      "\t Val. Loss: 0.317 |  Val. Acc: 16.67%\n",
      "Epoch: 14\n",
      "\tTrain Loss: 0.271 | Train Acc: 42.36%\n",
      "\t Val. Loss: 0.297 |  Val. Acc: 30.21%\n",
      "Epoch: 15\n",
      "\tTrain Loss: 0.269 | Train Acc: 41.67%\n",
      "\t Val. Loss: 0.295 |  Val. Acc: 23.96%\n",
      "Epoch: 16\n",
      "\tTrain Loss: 0.264 | Train Acc: 46.23%\n",
      "\t Val. Loss: 0.287 |  Val. Acc: 37.50%\n",
      "Epoch: 17\n",
      "\tTrain Loss: 0.261 | Train Acc: 49.21%\n",
      "\t Val. Loss: 0.293 |  Val. Acc: 34.38%\n",
      "Epoch: 18\n",
      "\tTrain Loss: 0.257 | Train Acc: 50.50%\n",
      "\t Val. Loss: 0.289 |  Val. Acc: 23.96%\n",
      "Epoch: 19\n",
      "\tTrain Loss: 0.254 | Train Acc: 58.73%\n",
      "\t Val. Loss: 0.279 |  Val. Acc: 35.42%\n",
      "Epoch: 20\n",
      "\tTrain Loss: 0.251 | Train Acc: 55.46%\n",
      "\t Val. Loss: 0.283 |  Val. Acc: 48.96%\n",
      "Epoch: 21\n",
      "\tTrain Loss: 0.255 | Train Acc: 53.47%\n",
      "\t Val. Loss: 0.284 |  Val. Acc: 36.46%\n",
      "Epoch: 22\n",
      "\tTrain Loss: 0.252 | Train Acc: 55.16%\n",
      "\t Val. Loss: 0.284 |  Val. Acc: 34.38%\n",
      "Epoch: 23\n",
      "\tTrain Loss: 0.250 | Train Acc: 59.72%\n",
      "\t Val. Loss: 0.303 |  Val. Acc: 26.04%\n",
      "Epoch: 24\n",
      "\tTrain Loss: 0.246 | Train Acc: 63.89%\n",
      "\t Val. Loss: 0.272 |  Val. Acc: 50.00%\n",
      "Epoch: 25\n",
      "\tTrain Loss: 0.244 | Train Acc: 69.35%\n",
      "\t Val. Loss: 0.277 |  Val. Acc: 39.58%\n",
      "Epoch: 26\n",
      "\tTrain Loss: 0.240 | Train Acc: 66.37%\n",
      "\t Val. Loss: 0.272 |  Val. Acc: 43.75%\n",
      "Epoch: 27\n",
      "\tTrain Loss: 0.237 | Train Acc: 71.23%\n",
      "\t Val. Loss: 0.276 |  Val. Acc: 43.75%\n",
      "Epoch: 28\n",
      "\tTrain Loss: 0.232 | Train Acc: 76.09%\n",
      "\t Val. Loss: 0.276 |  Val. Acc: 46.88%\n",
      "Epoch: 29\n",
      "\tTrain Loss: 0.235 | Train Acc: 72.92%\n",
      "\t Val. Loss: 0.272 |  Val. Acc: 45.83%\n",
      "Epoch: 30\n",
      "\tTrain Loss: 0.232 | Train Acc: 79.46%\n",
      "\t Val. Loss: 0.269 |  Val. Acc: 51.04%\n",
      "Epoch: 31\n",
      "\tTrain Loss: 0.230 | Train Acc: 77.38%\n",
      "\t Val. Loss: 0.266 |  Val. Acc: 50.00%\n",
      "Epoch: 32\n",
      "\tTrain Loss: 0.233 | Train Acc: 78.67%\n",
      "\t Val. Loss: 0.266 |  Val. Acc: 57.29%\n",
      "Epoch: 33\n",
      "\tTrain Loss: 0.229 | Train Acc: 78.97%\n",
      "\t Val. Loss: 0.270 |  Val. Acc: 43.75%\n",
      "Epoch: 34\n",
      "\tTrain Loss: 0.229 | Train Acc: 80.36%\n",
      "\t Val. Loss: 0.266 |  Val. Acc: 57.29%\n",
      "Epoch: 35\n",
      "\tTrain Loss: 0.226 | Train Acc: 82.44%\n",
      "\t Val. Loss: 0.266 |  Val. Acc: 50.00%\n",
      "Epoch: 36\n",
      "\tTrain Loss: 0.224 | Train Acc: 88.19%\n",
      "\t Val. Loss: 0.264 |  Val. Acc: 42.71%\n",
      "Epoch: 37\n",
      "\tTrain Loss: 0.227 | Train Acc: 79.56%\n",
      "\t Val. Loss: 0.259 |  Val. Acc: 51.04%\n",
      "Epoch: 38\n",
      "\tTrain Loss: 0.224 | Train Acc: 83.93%\n",
      "\t Val. Loss: 0.260 |  Val. Acc: 56.25%\n",
      "Epoch: 39\n",
      "\tTrain Loss: 0.219 | Train Acc: 89.29%\n",
      "\t Val. Loss: 0.262 |  Val. Acc: 52.08%\n",
      "Epoch: 40\n",
      "\tTrain Loss: 0.222 | Train Acc: 83.73%\n",
      "\t Val. Loss: 0.274 |  Val. Acc: 43.75%\n",
      "Epoch: 41\n",
      "\tTrain Loss: 0.222 | Train Acc: 84.72%\n",
      "\t Val. Loss: 0.260 |  Val. Acc: 51.04%\n",
      "Epoch: 42\n",
      "\tTrain Loss: 0.221 | Train Acc: 86.11%\n",
      "\t Val. Loss: 0.257 |  Val. Acc: 51.04%\n",
      "Epoch: 43\n",
      "\tTrain Loss: 0.222 | Train Acc: 89.58%\n",
      "\t Val. Loss: 0.255 |  Val. Acc: 59.38%\n",
      "Epoch: 44\n",
      "\tTrain Loss: 0.220 | Train Acc: 88.69%\n",
      "\t Val. Loss: 0.258 |  Val. Acc: 63.54%\n",
      "Epoch: 45\n",
      "\tTrain Loss: 0.218 | Train Acc: 89.38%\n",
      "\t Val. Loss: 0.255 |  Val. Acc: 56.25%\n",
      "Epoch: 46\n",
      "\tTrain Loss: 0.221 | Train Acc: 88.00%\n",
      "\t Val. Loss: 0.258 |  Val. Acc: 59.38%\n",
      "Epoch: 47\n",
      "\tTrain Loss: 0.216 | Train Acc: 92.86%\n",
      "\t Val. Loss: 0.253 |  Val. Acc: 67.71%\n",
      "Epoch: 48\n",
      "\tTrain Loss: 0.217 | Train Acc: 90.77%\n",
      "\t Val. Loss: 0.251 |  Val. Acc: 58.33%\n",
      "Epoch: 49\n",
      "\tTrain Loss: 0.216 | Train Acc: 91.17%\n",
      "\t Val. Loss: 0.259 |  Val. Acc: 60.42%\n",
      "Epoch: 50\n",
      "\tTrain Loss: 0.217 | Train Acc: 90.28%\n",
      "\t Val. Loss: 0.250 |  Val. Acc: 65.62%\n",
      "Epoch: 51\n",
      "\tTrain Loss: 0.215 | Train Acc: 93.45%\n",
      "\t Val. Loss: 0.253 |  Val. Acc: 64.58%\n",
      "Epoch: 52\n",
      "\tTrain Loss: 0.216 | Train Acc: 89.98%\n",
      "\t Val. Loss: 0.251 |  Val. Acc: 57.29%\n",
      "Epoch: 53\n",
      "\tTrain Loss: 0.215 | Train Acc: 94.05%\n",
      "\t Val. Loss: 0.243 |  Val. Acc: 68.75%\n",
      "Epoch: 54\n",
      "\tTrain Loss: 0.216 | Train Acc: 92.36%\n",
      "\t Val. Loss: 0.250 |  Val. Acc: 58.33%\n",
      "Epoch: 55\n",
      "\tTrain Loss: 0.216 | Train Acc: 92.26%\n",
      "\t Val. Loss: 0.240 |  Val. Acc: 65.62%\n",
      "Epoch: 56\n",
      "\tTrain Loss: 0.214 | Train Acc: 93.45%\n",
      "\t Val. Loss: 0.247 |  Val. Acc: 70.83%\n",
      "Epoch: 57\n",
      "\tTrain Loss: 0.213 | Train Acc: 93.45%\n",
      "\t Val. Loss: 0.243 |  Val. Acc: 66.67%\n",
      "Epoch: 58\n",
      "\tTrain Loss: 0.213 | Train Acc: 93.85%\n",
      "\t Val. Loss: 0.243 |  Val. Acc: 70.83%\n",
      "Epoch: 59\n",
      "\tTrain Loss: 0.211 | Train Acc: 94.94%\n",
      "\t Val. Loss: 0.246 |  Val. Acc: 53.12%\n",
      "Epoch: 60\n",
      "\tTrain Loss: 0.211 | Train Acc: 95.54%\n",
      "\t Val. Loss: 0.241 |  Val. Acc: 70.83%\n",
      "Epoch: 61\n",
      "\tTrain Loss: 0.210 | Train Acc: 96.43%\n",
      "\t Val. Loss: 0.239 |  Val. Acc: 70.83%\n",
      "Epoch: 62\n",
      "\tTrain Loss: 0.209 | Train Acc: 97.62%\n",
      "\t Val. Loss: 0.242 |  Val. Acc: 60.42%\n",
      "Epoch: 63\n",
      "\tTrain Loss: 0.209 | Train Acc: 97.62%\n",
      "\t Val. Loss: 0.246 |  Val. Acc: 58.33%\n",
      "Epoch: 64\n",
      "\tTrain Loss: 0.209 | Train Acc: 97.92%\n",
      "\t Val. Loss: 0.240 |  Val. Acc: 70.83%\n",
      "Epoch: 65\n",
      "\tTrain Loss: 0.211 | Train Acc: 95.63%\n",
      "\t Val. Loss: 0.242 |  Val. Acc: 59.38%\n",
      "Epoch: 66\n",
      "\tTrain Loss: 0.209 | Train Acc: 95.24%\n",
      "\t Val. Loss: 0.235 |  Val. Acc: 73.96%\n",
      "Epoch: 67\n",
      "\tTrain Loss: 0.207 | Train Acc: 98.51%\n",
      "\t Val. Loss: 0.234 |  Val. Acc: 72.92%\n",
      "Epoch: 68\n",
      "\tTrain Loss: 0.206 | Train Acc: 98.21%\n",
      "\t Val. Loss: 0.235 |  Val. Acc: 76.04%\n",
      "Epoch: 69\n",
      "\tTrain Loss: 0.207 | Train Acc: 97.92%\n",
      "\t Val. Loss: 0.229 |  Val. Acc: 79.17%\n",
      "Epoch: 70\n",
      "\tTrain Loss: 0.209 | Train Acc: 95.34%\n",
      "\t Val. Loss: 0.231 |  Val. Acc: 72.92%\n",
      "Epoch: 71\n",
      "\tTrain Loss: 0.208 | Train Acc: 97.62%\n",
      "\t Val. Loss: 0.236 |  Val. Acc: 78.12%\n",
      "Epoch: 72\n",
      "\tTrain Loss: 0.208 | Train Acc: 96.73%\n",
      "\t Val. Loss: 0.233 |  Val. Acc: 77.08%\n",
      "Epoch: 73\n",
      "\tTrain Loss: 0.208 | Train Acc: 94.94%\n",
      "\t Val. Loss: 0.230 |  Val. Acc: 78.12%\n",
      "Epoch: 74\n",
      "\tTrain Loss: 0.207 | Train Acc: 98.21%\n",
      "\t Val. Loss: 0.235 |  Val. Acc: 69.79%\n",
      "Epoch: 75\n",
      "\tTrain Loss: 0.207 | Train Acc: 98.81%\n",
      "\t Val. Loss: 0.236 |  Val. Acc: 64.58%\n",
      "Epoch: 76\n",
      "\tTrain Loss: 0.207 | Train Acc: 97.62%\n",
      "\t Val. Loss: 0.237 |  Val. Acc: 63.54%\n",
      "Epoch: 77\n",
      "\tTrain Loss: 0.208 | Train Acc: 97.62%\n",
      "\t Val. Loss: 0.231 |  Val. Acc: 75.00%\n",
      "Epoch: 78\n",
      "\tTrain Loss: 0.206 | Train Acc: 98.51%\n",
      "\t Val. Loss: 0.233 |  Val. Acc: 73.96%\n",
      "Epoch: 79\n",
      "\tTrain Loss: 0.205 | Train Acc: 99.11%\n",
      "\t Val. Loss: 0.229 |  Val. Acc: 76.04%\n",
      "Epoch: 80\n",
      "\tTrain Loss: 0.205 | Train Acc: 98.61%\n",
      "\t Val. Loss: 0.232 |  Val. Acc: 61.46%\n",
      "Epoch: 81\n",
      "\tTrain Loss: 0.205 | Train Acc: 98.81%\n",
      "\t Val. Loss: 0.224 |  Val. Acc: 77.08%\n",
      "Epoch: 82\n",
      "\tTrain Loss: 0.205 | Train Acc: 98.81%\n",
      "\t Val. Loss: 0.229 |  Val. Acc: 73.96%\n",
      "Epoch: 83\n",
      "\tTrain Loss: 0.205 | Train Acc: 99.11%\n",
      "\t Val. Loss: 0.234 |  Val. Acc: 66.67%\n",
      "Epoch: 84\n",
      "\tTrain Loss: 0.206 | Train Acc: 97.92%\n",
      "\t Val. Loss: 0.224 |  Val. Acc: 81.25%\n",
      "Epoch: 85\n",
      "\tTrain Loss: 0.206 | Train Acc: 98.51%\n",
      "\t Val. Loss: 0.225 |  Val. Acc: 78.12%\n",
      "Epoch: 86\n",
      "\tTrain Loss: 0.204 | Train Acc: 99.40%\n",
      "\t Val. Loss: 0.223 |  Val. Acc: 77.08%\n",
      "Epoch: 87\n",
      "\tTrain Loss: 0.205 | Train Acc: 98.81%\n",
      "\t Val. Loss: 0.224 |  Val. Acc: 81.25%\n",
      "Epoch: 88\n",
      "\tTrain Loss: 0.204 | Train Acc: 99.11%\n",
      "\t Val. Loss: 0.221 |  Val. Acc: 78.12%\n",
      "Epoch: 89\n",
      "\tTrain Loss: 0.206 | Train Acc: 98.31%\n",
      "\t Val. Loss: 0.229 |  Val. Acc: 71.88%\n",
      "Epoch: 90\n",
      "\tTrain Loss: 0.207 | Train Acc: 98.21%\n",
      "\t Val. Loss: 0.225 |  Val. Acc: 77.08%\n",
      "Epoch: 91\n",
      "\tTrain Loss: 0.206 | Train Acc: 99.11%\n",
      "\t Val. Loss: 0.228 |  Val. Acc: 78.12%\n",
      "Epoch: 92\n",
      "\tTrain Loss: 0.205 | Train Acc: 97.62%\n",
      "\t Val. Loss: 0.223 |  Val. Acc: 79.17%\n",
      "Epoch: 93\n",
      "\tTrain Loss: 0.205 | Train Acc: 99.70%\n",
      "\t Val. Loss: 0.225 |  Val. Acc: 71.88%\n",
      "Epoch: 94\n",
      "\tTrain Loss: 0.205 | Train Acc: 99.40%\n",
      "\t Val. Loss: 0.227 |  Val. Acc: 77.08%\n",
      "Epoch: 95\n",
      "\tTrain Loss: 0.204 | Train Acc: 99.11%\n",
      "\t Val. Loss: 0.241 |  Val. Acc: 66.67%\n",
      "Epoch: 96\n",
      "\tTrain Loss: 0.204 | Train Acc: 99.70%\n",
      "\t Val. Loss: 0.224 |  Val. Acc: 84.38%\n",
      "Epoch: 97\n",
      "\tTrain Loss: 0.204 | Train Acc: 98.81%\n",
      "\t Val. Loss: 0.228 |  Val. Acc: 75.00%\n",
      "Epoch: 98\n",
      "\tTrain Loss: 0.205 | Train Acc: 97.92%\n",
      "\t Val. Loss: 0.227 |  Val. Acc: 72.92%\n",
      "Epoch: 99\n",
      "\tTrain Loss: 0.204 | Train Acc: 99.11%\n",
      "\t Val. Loss: 0.224 |  Val. Acc: 79.17%\n",
      "Epoch: 100\n",
      "\tTrain Loss: 0.204 | Train Acc: 99.70%\n",
      "\t Val. Loss: 0.235 |  Val. Acc: 68.75%\n",
      "Training: finish\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf')\n",
    "best_val_acc = 0.0\n",
    "for epc in range(100):\n",
    "\n",
    "    train_loss, train_acc = train(net, trainloader, optimizer, criterion, num_classes, device)\n",
    "    val_loss, val_acc = val(net, testloader, criterion, num_classes, device)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        #torch.save(model.state_dict(), os.path.join(config[\"save_dir\"], config[\"date\"], \"model.pt\"))\n",
    "    \n",
    "    print(f'Epoch: {epc+1:02}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {val_loss:.3f} |  Val. Acc: {val_acc*100:.2f}%')\n",
    "    \n",
    "print(\"Training: finish\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
